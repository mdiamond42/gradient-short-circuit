{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdiamond42/gradient-short-circuit/blob/main/GSC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB55MVKug9Bb"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "t-T3Ma7k7423",
        "outputId": "09f1d2a1-9c13-4f5d-9832-5e6567461b50"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " step 2500000/2500000 | K=1\n",
            "   acc=0.96090.0012 | metric=0.9603\n",
            "   BEST: 0.96140.0011 @ 2495000\n",
            "\n",
            " Training complete!\n",
            "Final: /content/drive/My Drive/GradientShortCircuit_Experiments/GSC_37dim_96hidden_K1_g2proxy_200k.pth\n",
            "Best:  /content/drive/My Drive/GradientShortCircuit_Experiments/GSC_37dim_96hidden_K1_g2proxy_200k_BEST.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from google.colab import drive\n",
        "\n",
        "# ============================================================\n",
        "# GSC with 96-HIDDEN STUDENT + FINE-GRAINED FEATURES (37-dim)\n",
        "#\n",
        "# Student: 784  96  10 (1.5x capacity vs 64-hidden baseline)\n",
        "#\n",
        "# Hessian: Simple g^2 proxy (faster than Hutchinson)\n",
        "#\n",
        "# New features for better resolution in low-loss regions:\n",
        "#   1. Distance from current weight center (per-weight)\n",
        "#   2. Center drift from initialization (scalar)\n",
        "#   3. Raw gradient magnitude (log1p, unsquashed)\n",
        "#   4. Curvature-to-gradient ratio (Hessian/grad)\n",
        "#\n",
        "# Hypothesis: Soft-squashed features lose fine detail when\n",
        "# weights are near optimum. Raw/ratio features preserve the\n",
        "# signal needed for precise refinement.\n",
        "#\n",
        "# Total features: 33 (baseline) + 4 (new) = 37\n",
        "# K schedule: K=1 always (no K=2 switch)\n",
        "# ============================================================\n",
        "\n",
        "torch.set_grad_enabled(True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\" Running on {device}\")\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Helpers\n",
        "# ============================================================\n",
        "def atomic_save(state, filepath):\n",
        "    tmp = filepath + \".tmp\"\n",
        "    torch.save(state, tmp)\n",
        "    os.replace(tmp, filepath)\n",
        "\n",
        "def loss_stats(loss_history, window=500):\n",
        "    if len(loss_history) == 0:\n",
        "        return float(\"nan\"), float(\"nan\")\n",
        "    last = float(loss_history[-1])\n",
        "    w = int(min(window, len(loss_history)))\n",
        "    avg = float(np.mean(loss_history[-w:]))\n",
        "    return last, avg\n",
        "\n",
        "@torch.no_grad()\n",
        "def update_ema(ema_model, model, decay):\n",
        "    for p_ema, p in zip(ema_model.parameters(), model.parameters()):\n",
        "        p_ema.mul_(decay).add_(p, alpha=1 - decay)\n",
        "\n",
        "def soft_squash(x, eps=1e-8):\n",
        "    s = x.detach().abs().median().clamp(min=eps)\n",
        "    return torch.tanh(x / (3.0 * s))\n",
        "\n",
        "def signed_log1p(x):\n",
        "    return torch.sign(x) * torch.log1p(x.abs())\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Data\n",
        "# ============================================================\n",
        "class FastMNISTLoader:\n",
        "    def __init__(self, device):\n",
        "        print(\" Pre-loading MNIST to GPU VRAM...\")\n",
        "        self.device = device\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "        train_data = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
        "        test_data  = datasets.MNIST(\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "        self.train_x = train_data.data.view(-1, 784).float().to(device) / 255.0\n",
        "        self.train_y = train_data.targets.to(device)\n",
        "        self.test_x  = test_data.data.view(-1, 784).float().to(device) / 255.0\n",
        "        self.test_y  = test_data.targets.to(device)\n",
        "\n",
        "        self.train_x = (self.train_x - 0.1307) / 0.3081\n",
        "        self.test_x  = (self.test_x  - 0.1307) / 0.3081\n",
        "\n",
        "        self.num_train = self.train_x.shape[0]\n",
        "\n",
        "    def sample_train(self, batch_size):\n",
        "        idx = torch.randint(0, self.num_train, (batch_size,), device=self.device)\n",
        "        return self.train_x[idx], self.train_y[idx]\n",
        "\n",
        "    def get_full_test(self):\n",
        "        return self.test_x, self.test_y\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Student\n",
        "# ============================================================\n",
        "class MNISTStudent(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(784, 96),  # Changed from 64 to 96\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(96, 10)    # Changed from 64 to 96\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def functional_forward(self, x, params):\n",
        "        x = F.linear(x, params[0], params[1])\n",
        "        x = F.relu(x)\n",
        "        x = F.linear(x, params[2], params[3])\n",
        "        return x\n",
        "\n",
        "def reset_student(net):\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Probe + curvature\n",
        "# ============================================================\n",
        "def _rademacher_like(t: torch.Tensor) -> torch.Tensor:\n",
        "    return (torch.randint(0, 2, t.shape, device=t.device, dtype=torch.int8) * 2 - 1).to(dtype=t.dtype)\n",
        "\n",
        "def compute_probe_and_curv_fullcombo(\n",
        "    student, params, x_support, y_support,\n",
        "    probe_lr=0.02, jerk_scale=1e-4,\n",
        "    hutchinson_samples=3\n",
        "):\n",
        "    p0 = [p.detach().clone().requires_grad_(True) for p in params]\n",
        "\n",
        "    logits1 = student.functional_forward(x_support, p0)\n",
        "    loss1 = F.cross_entropy(logits1, y_support)\n",
        "\n",
        "    grads1 = torch.autograd.grad(loss1, p0, create_graph=False)\n",
        "    g1 = [g.detach() for g in grads1]\n",
        "\n",
        "    # Simple Hessian proxy: g^2 (much faster than Hutchinson)\n",
        "    h_diag = [g.pow(2) for g in g1]\n",
        "\n",
        "    # Probe step 1\n",
        "    p1 = [(pp - probe_lr * gg).detach().requires_grad_(True) for pp, gg in zip(p0, g1)]\n",
        "    logits2 = student.functional_forward(x_support, p1)\n",
        "    loss2 = F.cross_entropy(logits2, y_support)\n",
        "    deltaL1 = float((loss2 - loss1).detach().item())\n",
        "    grads2 = torch.autograd.grad(loss2, p1, create_graph=False)\n",
        "    g2 = [g.detach() for g in grads2]\n",
        "    accel = [g2i - g1i for g1i, g2i in zip(g1, g2)]\n",
        "\n",
        "    # cos(g1,g2)\n",
        "    eps = 1e-12\n",
        "    dot = torch.zeros((), device=g1[0].device)\n",
        "    n1  = torch.zeros((), device=g1[0].device)\n",
        "    n2  = torch.zeros((), device=g1[0].device)\n",
        "    for a, b in zip(g1, g2):\n",
        "        aa = a.reshape(-1)\n",
        "        bb = b.reshape(-1)\n",
        "        dot = dot + (aa * bb).sum()\n",
        "        n1  = n1  + (aa * aa).sum()\n",
        "        n2  = n2  + (bb * bb).sum()\n",
        "    cosv = float((dot / (torch.sqrt(n1).clamp_min(eps) * torch.sqrt(n2).clamp_min(eps))).detach().item())\n",
        "\n",
        "    # Probe step 2\n",
        "    p2 = [(pp - probe_lr * gg).detach().requires_grad_(True) for pp, gg in zip(p1, g2)]\n",
        "    logits3 = student.functional_forward(x_support, p2)\n",
        "    loss3 = F.cross_entropy(logits3, y_support)\n",
        "    deltaL2_raw = float((loss3 - loss2).detach().item())\n",
        "    grads3 = torch.autograd.grad(loss3, p2, create_graph=False)\n",
        "    g3 = [g.detach() for g in grads3]\n",
        "\n",
        "    jerk = [float(jerk_scale) * (g3i - 2.0*g2i + g1i) for g1i, g2i, g3i in zip(g1, g2, g3)]\n",
        "    return g1, accel, jerk, h_diag, deltaL1, deltaL2_raw, cosv\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Hypertuner (37-dim with fine-grained features)\n",
        "# ============================================================\n",
        "class FineGrainedHypertuner37(nn.Module):\n",
        "    def __init__(self, num_layers=2, deltal_scale=2.0, deltal2_scale=5.0, alpha_head_max=0.5):\n",
        "        super().__init__()\n",
        "        self.layer_embeddings = nn.Embedding(num_layers, 10)\n",
        "\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(37, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        self.alpha_head = nn.Sequential(\n",
        "            nn.Linear(37, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            self.alpha_head[-1].bias.fill_(0.0)\n",
        "\n",
        "        self.deltal_scale = float(deltal_scale)\n",
        "        self.deltal2_scale = float(deltal2_scale)\n",
        "        self.alpha_head_max = float(alpha_head_max)\n",
        "\n",
        "    def forward(self, weight, grad_g1, grad_accel, grad_jerk, h_diag, layer_idx,\n",
        "                support_ce_scalar, deltaL1_scalar, deltaL2_scalar, cos_scalar):\n",
        "        batch_size = weight.numel()\n",
        "        w_flat = weight.reshape(-1, 1)\n",
        "\n",
        "        # Existing squashed features\n",
        "        g_flat = soft_squash(grad_g1).reshape(-1, 1)\n",
        "        a_flat = soft_squash(grad_accel).reshape(-1, 1)\n",
        "        j_flat = soft_squash(grad_jerk).reshape(-1, 1)\n",
        "        h_feat = soft_squash(signed_log1p(h_diag)).reshape(-1, 1)\n",
        "\n",
        "        dL1_feat = torch.tanh(torch.full((batch_size, 1), float(deltaL1_scalar) * self.deltal_scale,\n",
        "                                         device=weight.device, dtype=w_flat.dtype))\n",
        "        dL2_feat = torch.tanh(torch.full((batch_size, 1), float(deltaL2_scalar) * self.deltal2_scale,\n",
        "                                         device=weight.device, dtype=w_flat.dtype))\n",
        "\n",
        "        cos_feat = torch.full((batch_size, 1), float(cos_scalar), device=weight.device, dtype=w_flat.dtype)\n",
        "        ce_feat  = torch.full((batch_size, 1), float(support_ce_scalar), device=weight.device, dtype=w_flat.dtype)\n",
        "\n",
        "        # Magnitude features (log1p of median)\n",
        "        log_mag_g1    = float(torch.log1p(grad_g1.detach().abs().median()).item())\n",
        "        log_mag_accel = float(torch.log1p(grad_accel.detach().abs().median()).item())\n",
        "        log_mag_jerk  = float(torch.log1p(grad_jerk.detach().abs().median()).item())\n",
        "        log_mag_h     = float(torch.log1p(h_diag.detach().abs().median()).item())\n",
        "        mag_features = torch.tensor([log_mag_g1, log_mag_accel, log_mag_jerk, log_mag_h],\n",
        "                                    device=weight.device, dtype=w_flat.dtype).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # === NEW FINE-GRAINED FEATURES ===\n",
        "\n",
        "        # 1. Current center of weight distribution\n",
        "        current_center = float(weight.mean().item())\n",
        "\n",
        "        # 2. Distance from current center (per-weight, preserves fine detail)\n",
        "        dist_current_center = torch.log1p((w_flat - current_center).abs())\n",
        "\n",
        "        # 3. Center drift from initialization (how much distribution shifted)\n",
        "        center_drift = torch.log1p(torch.abs(torch.tensor(current_center, device=weight.device)))\n",
        "        center_drift_feat = torch.full((batch_size, 1), float(center_drift.item()),\n",
        "                                      device=weight.device, dtype=w_flat.dtype)\n",
        "\n",
        "        # 4. Raw gradient magnitude (unsquashed, preserves scale in low-loss regions)\n",
        "        grad_mag_raw = torch.log1p(grad_g1.abs()).reshape(-1, 1)\n",
        "\n",
        "        # 5. Curvature-to-gradient ratio (indicates proximity to minimum)\n",
        "        # High ratio = small gradients, high curvature = near minimum\n",
        "        # Low ratio = large gradients, low curvature = far from minimum\n",
        "        h_to_g_ratio = soft_squash(h_diag.abs() / (grad_g1.abs() + 1e-8)).reshape(-1, 1)\n",
        "\n",
        "        # Geometry features (same as before)\n",
        "        if weight.dim() > 1:\n",
        "            rows, cols = weight.shape\n",
        "            val_in, val_out = float(np.log1p(cols)), float(np.log1p(rows))\n",
        "\n",
        "            r_idx = torch.arange(rows, device=weight.device)\n",
        "            c_idx = torch.arange(cols, device=weight.device)\n",
        "            r = r_idx.repeat_interleave(cols).float().reshape(-1, 1) / rows\n",
        "            c = c_idx.repeat(rows).float().reshape(-1, 1) / cols\n",
        "\n",
        "            center_row = (rows - 1) / 2.0\n",
        "            center_col = (cols - 1) / 2.0\n",
        "            rr = r_idx.repeat_interleave(cols).float()\n",
        "            cc = c_idx.repeat(rows).float()\n",
        "            dist = torch.sqrt((rr - center_row) ** 2 + (cc - center_col) ** 2)\n",
        "            max_dist = torch.sqrt(torch.tensor(center_row**2 + center_col**2, device=weight.device, dtype=dist.dtype)).clamp(min=1e-8)\n",
        "            dist_center = (dist / max_dist).reshape(-1, 1)\n",
        "\n",
        "            fan_in = max(int(cols), 1)\n",
        "            sigma = torch.sqrt(torch.tensor(2.0 / fan_in, device=weight.device, dtype=w_flat.dtype)).clamp(min=1e-8)\n",
        "            dist_origin = torch.log1p(w_flat.abs() / sigma)\n",
        "\n",
        "            row_norms = grad_g1.norm(dim=1, keepdim=True)\n",
        "            col_norms = grad_g1.norm(dim=0, keepdim=True).t()\n",
        "            row_feat = soft_squash(row_norms.repeat_interleave(cols, dim=0))\n",
        "            col_feat = soft_squash(col_norms.repeat(rows, 1))\n",
        "\n",
        "            g_mean = g_flat.mean().expand(batch_size, 1)\n",
        "            g_std  = g_flat.std().expand(batch_size, 1)\n",
        "\n",
        "            t_in  = torch.full((batch_size, 1), val_in, device=weight.device)\n",
        "            t_out = torch.full((batch_size, 1), val_out, device=weight.device)\n",
        "\n",
        "        else:\n",
        "            rows = weight.shape[0]\n",
        "            r_idx = torch.arange(rows, device=weight.device)\n",
        "            r = r_idx.float().reshape(-1, 1) / rows\n",
        "            c = torch.zeros_like(r)\n",
        "\n",
        "            center = (rows - 1) / 2.0\n",
        "            dist = (r_idx.float() - center).abs()\n",
        "            max_dist = torch.tensor(center, device=weight.device, dtype=dist.dtype).clamp(min=1e-8)\n",
        "            dist_center = (dist / max_dist).reshape(-1, 1)\n",
        "\n",
        "            fan_in = 784 if layer_idx == 0 else 96  # Changed from 64 to 96\n",
        "            sigma = torch.sqrt(torch.tensor(2.0 / fan_in, device=weight.device, dtype=w_flat.dtype)).clamp(min=1e-8)\n",
        "            dist_origin = torch.log1p(w_flat.abs() / sigma)\n",
        "\n",
        "            row_feat = soft_squash(grad_g1.abs().reshape(-1, 1))\n",
        "            col_feat = torch.zeros_like(row_feat)\n",
        "\n",
        "            g_mean = g_flat.mean().expand(batch_size, 1)\n",
        "            g_std  = g_flat.std().expand(batch_size, 1)\n",
        "\n",
        "            t_in  = torch.zeros((batch_size, 1), device=weight.device)\n",
        "            t_out = torch.full((batch_size, 1), float(np.log1p(rows)), device=weight.device)\n",
        "\n",
        "        l_emb = self.layer_embeddings(torch.tensor([layer_idx], device=weight.device)).expand(batch_size, -1)\n",
        "\n",
        "        # Concatenate all 37 features\n",
        "        inputs = torch.cat([\n",
        "            w_flat,                    # 1\n",
        "            g_flat,                    # 1\n",
        "            a_flat,                    # 1\n",
        "            j_flat,                    # 1\n",
        "            l_emb,                     # 10\n",
        "            g_mean, g_std,             # 2\n",
        "            t_in, t_out,               # 2\n",
        "            r, c,                      # 2\n",
        "            dist_center,               # 1\n",
        "            dist_origin,               # 1\n",
        "            row_feat, col_feat,        # 2\n",
        "            h_feat,                    # 1\n",
        "            dL1_feat, dL2_feat,        # 2\n",
        "            cos_feat, ce_feat,         # 2\n",
        "            mag_features,              # 4\n",
        "            # === NEW (4) ===\n",
        "            dist_current_center,       # 1 - distance from moving center\n",
        "            center_drift_feat,         # 1 - how much center drifted\n",
        "            grad_mag_raw,              # 1 - raw gradient magnitude\n",
        "            h_to_g_ratio,              # 1 - curvature/gradient ratio\n",
        "        ], dim=1)  # Total: 37\n",
        "\n",
        "        delta = 0.5 * torch.tanh(self.predictor(inputs))\n",
        "        p_full = (w_flat + delta).view_as(weight)\n",
        "\n",
        "        a0 = self.alpha_head_max * torch.sigmoid(self.alpha_head(inputs))\n",
        "        return p_full, a0.view_as(weight)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Eval\n",
        "# ============================================================\n",
        "def eval_step1_multitrial(ht, fast_loader, student, cfg, trials=10):\n",
        "    ht.eval()\n",
        "    x_test, y_test = fast_loader.get_full_test()\n",
        "\n",
        "    accs = []\n",
        "    a0_means = []\n",
        "\n",
        "    for _ in range(trials):\n",
        "        reset_student(student)\n",
        "        student.eval()\n",
        "        params0 = [p.detach().clone() for p in student.parameters()]\n",
        "        x_s, y_s = fast_loader.sample_train(cfg[\"BATCH\"])\n",
        "\n",
        "        jscale = cfg[\"JERK_EVAL_SCALE\"]\n",
        "\n",
        "        with torch.enable_grad():\n",
        "            g1, accel, jerk, h0, dL1, dL2, cosv = compute_probe_and_curv_fullcombo(\n",
        "                student, params0, x_s, y_s,\n",
        "                probe_lr=cfg[\"PROBE_LR\"],\n",
        "                jerk_scale=jscale,\n",
        "                hutchinson_samples=cfg[\"HUTCHINSON_SAMPLES\"]\n",
        "            )\n",
        "\n",
        "        ce0 = float(F.cross_entropy(student.functional_forward(x_s, params0), y_s).item())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            step1_full, alpha0 = [], []\n",
        "            for i, (p, g1i, ai, ji, hi) in enumerate(zip(params0, g1, accel, jerk, h0)):\n",
        "                pf, a = ht(p, g1i, ai, ji, hi, i // 2, ce0, dL1, dL2, cosv)\n",
        "                step1_full.append(pf)\n",
        "                alpha0.append(a)\n",
        "\n",
        "            step1 = []\n",
        "            a_used = []\n",
        "            for i, (p, pf, a) in enumerate(zip(params0, step1_full, alpha0)):\n",
        "                au = torch.clamp(a * cfg[\"MULTS\"][i], 0.0, cfg[\"ALPHA_APPLY_MAX\"])\n",
        "                step1.append(p + au * (pf - p))\n",
        "                a_used.append(float(au.mean().item()))\n",
        "\n",
        "            acc = (student.functional_forward(x_test, step1).argmax(1) == y_test).float().mean().item()\n",
        "\n",
        "        accs.append(acc)\n",
        "        a0_means.append(float(np.mean(a_used)))\n",
        "\n",
        "    return float(np.mean(accs)), float(np.std(accs)), float(np.mean(a0_means))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Train\n",
        "# ============================================================\n",
        "def train_fine_grained(cfg):\n",
        "    experiment_dir = cfg[\"EXPERIMENT_DIR\"]\n",
        "    os.makedirs(experiment_dir, exist_ok=True)\n",
        "\n",
        "    RUN_TAG = cfg[\"RUN_TAG\"]\n",
        "    CKPT_FILE = os.path.join(experiment_dir, f\"{RUN_TAG}.pth\")\n",
        "    BEST_FILE = os.path.join(experiment_dir, f\"{RUN_TAG}_BEST.pth\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*120)\n",
        "    print(\" 96-HIDDEN STUDENT + FINE-GRAINED FEATURES (37-dim)\")\n",
        "    print(f\"RUN_TAG: {RUN_TAG}\")\n",
        "    print(f\"Target steps: {cfg['TARGET_STEPS']}\")\n",
        "    print(\"-\"*120)\n",
        "    print(\"Student: 784  96  10 (76,330 params, 1.5x vs 64-hidden baseline)\")\n",
        "    print(\"Hessian: g^2 proxy (faster than Hutchinson)\")\n",
        "    print(\"New features for low-loss resolution:\")\n",
        "    print(\"  1. Distance from current weight center (per-weight)\")\n",
        "    print(\"  2. Center drift from initialization (scalar)\")\n",
        "    print(\"  3. Raw gradient magnitude (unsquashed)\")\n",
        "    print(\"  4. Curvature-to-gradient ratio (H/g)\")\n",
        "    print(\"K schedule: K=1 always\")\n",
        "    print(\"=\"*120 + \"\\n\")\n",
        "\n",
        "    fast_loader = FastMNISTLoader(device)\n",
        "    student = MNISTStudent().to(device)\n",
        "\n",
        "    ht = FineGrainedHypertuner37(\n",
        "        num_layers=2,\n",
        "        deltal_scale=cfg[\"DELTAL_SCALE\"],\n",
        "        deltal2_scale=cfg[\"DELTAL2_SCALE\"],\n",
        "        alpha_head_max=cfg[\"ALPHA_HEAD_MAX\"],\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(ht.parameters(), lr=cfg[\"LR\"])\n",
        "\n",
        "    ht_ema = None\n",
        "    if cfg[\"USE_EMA\"]:\n",
        "        ht_ema = FineGrainedHypertuner37(\n",
        "            num_layers=2,\n",
        "            deltal_scale=cfg[\"DELTAL_SCALE\"],\n",
        "            deltal2_scale=cfg[\"DELTAL2_SCALE\"],\n",
        "            alpha_head_max=cfg[\"ALPHA_HEAD_MAX\"],\n",
        "        ).to(device)\n",
        "        ht_ema.load_state_dict(ht.state_dict())\n",
        "\n",
        "    start_step = 0\n",
        "    loss_history = []\n",
        "    acc_history = []\n",
        "    best_acc_mean = 0.0\n",
        "    best_acc_std = 0.0\n",
        "    best_step = 0\n",
        "    best_metric = -1e9\n",
        "\n",
        "    # Resume if exists\n",
        "    if os.path.exists(CKPT_FILE):\n",
        "        print(f\" Resuming from: {CKPT_FILE}\")\n",
        "        ckpt = torch.load(CKPT_FILE, map_location=device)\n",
        "        ht.load_state_dict(ckpt[\"model_state\"])\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "        if cfg[\"USE_EMA\"] and \"ema_state\" in ckpt:\n",
        "            ht_ema.load_state_dict(ckpt[\"ema_state\"])\n",
        "        start_step = ckpt[\"step\"]\n",
        "        loss_history = ckpt.get(\"loss_history\", [])\n",
        "        acc_history = ckpt.get(\"acc_history\", [])\n",
        "        best_acc_mean = ckpt.get(\"best_acc_mean\", 0.0)\n",
        "        best_acc_std = ckpt.get(\"best_acc_std\", 0.0)\n",
        "        best_step = ckpt.get(\"best_step\", 0)\n",
        "        best_metric = ckpt.get(\"best_metric\", best_acc_mean)\n",
        "        print(f\" Resumed at step {start_step}\\n\")\n",
        "\n",
        "    ht.train()\n",
        "    if cfg[\"USE_EMA\"]:\n",
        "        ht_ema.train()\n",
        "\n",
        "    def jerk_scale_at(step):\n",
        "        if step <= 0:\n",
        "            return 1.0\n",
        "        val = float(np.exp(-cfg[\"JERK_DECAY_RATE\"] * float(step)))\n",
        "        return max(cfg[\"MIN_JERK_SCALE\"], val)\n",
        "\n",
        "    def k_at(step):\n",
        "        \"\"\"K=1 always (pure K=1 experiment)\"\"\"\n",
        "        return 1\n",
        "\n",
        "    current_step = start_step\n",
        "\n",
        "    while current_step < cfg[\"TARGET_STEPS\"]:\n",
        "        burst_end = min(current_step + cfg[\"BURST\"], cfg[\"TARGET_STEPS\"])\n",
        "        t0 = time.time()\n",
        "\n",
        "        for step_idx in range(current_step, burst_end):\n",
        "            jscale = jerk_scale_at(step_idx)\n",
        "            K = k_at(step_idx)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            for _ in range(K):\n",
        "                x_s, y_s = fast_loader.sample_train(cfg[\"BATCH\"])\n",
        "                x_q, y_q = fast_loader.sample_train(cfg[\"BATCH\"])\n",
        "\n",
        "                reset_student(student)\n",
        "                student.train()\n",
        "                params0 = [p.detach().clone() for p in student.parameters()]\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    ce0 = float(F.cross_entropy(student.functional_forward(x_s, params0), y_s).item())\n",
        "\n",
        "                with torch.enable_grad():\n",
        "                    g1, accel, jerk, h0, dL1, dL2, cosv = compute_probe_and_curv_fullcombo(\n",
        "                        student, params0, x_s, y_s,\n",
        "                        probe_lr=cfg[\"PROBE_LR\"],\n",
        "                        jerk_scale=jscale,\n",
        "                        hutchinson_samples=cfg[\"HUTCHINSON_SAMPLES\"]\n",
        "                    )\n",
        "\n",
        "                step1_full, alpha0 = [], []\n",
        "                for i, (p, g1i, ai, ji, hi) in enumerate(zip(params0, g1, accel, jerk, h0)):\n",
        "                    pf, a = ht(p, g1i, ai, ji, hi, i // 2, ce0, dL1, dL2, cosv)\n",
        "                    step1_full.append(pf)\n",
        "                    alpha0.append(a)\n",
        "\n",
        "                step1 = []\n",
        "                for i, (p, pf, a) in enumerate(zip(params0, step1_full, alpha0)):\n",
        "                    mult = cfg[\"MULTS\"][i]\n",
        "                    au = torch.clamp(a * mult, 0.0, cfg[\"ALPHA_APPLY_MAX\"])\n",
        "                    step1.append(p + au * (pf - p))\n",
        "\n",
        "                ce1_q = F.cross_entropy(student.functional_forward(x_q, step1), y_q)\n",
        "                (ce1_q / float(K)).backward()\n",
        "                loss_history.append(float(ce1_q.item()))\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(ht.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            if cfg[\"USE_EMA\"]:\n",
        "                update_ema(ht_ema, ht, cfg[\"EMA_DECAY\"])\n",
        "\n",
        "        current_step = burst_end\n",
        "\n",
        "        # Save checkpoint\n",
        "        ckpt_out = {\n",
        "            \"step\": current_step,\n",
        "            \"model_state\": ht.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"ema_state\": ht_ema.state_dict() if cfg[\"USE_EMA\"] else None,\n",
        "            \"loss_history\": loss_history,\n",
        "            \"acc_history\": acc_history,\n",
        "            \"best_acc_mean\": best_acc_mean,\n",
        "            \"best_acc_std\": best_acc_std,\n",
        "            \"best_step\": best_step,\n",
        "            \"best_metric\": best_metric,\n",
        "            \"config\": cfg,\n",
        "        }\n",
        "        atomic_save(ckpt_out, CKPT_FILE)\n",
        "\n",
        "        # Eval\n",
        "        if (current_step % cfg[\"EVAL_EVERY\"] == 0) or (current_step == cfg[\"TARGET_STEPS\"]):\n",
        "            loss_last, loss_ma500 = loss_stats(loss_history, window=500)\n",
        "\n",
        "            eval_model = ht_ema if cfg[\"USE_EMA\"] else ht\n",
        "            acc_mean, acc_std, a0_mean = eval_step1_multitrial(eval_model, fast_loader, student, cfg, trials=cfg[\"EVAL_TRIALS\"])\n",
        "            metric = acc_mean - 0.5 * acc_std\n",
        "\n",
        "            acc_history.append({\n",
        "                \"step\": current_step,\n",
        "                \"acc_step1_mean\": acc_mean,\n",
        "                \"acc_step1_std\": acc_std,\n",
        "                \"alpha0_mean\": a0_mean,\n",
        "                \"metric\": metric,\n",
        "                \"k\": k_at(max(0, current_step - 1)),\n",
        "            })\n",
        "\n",
        "            if metric > best_metric:\n",
        "                best_metric = metric\n",
        "                best_acc_mean = acc_mean\n",
        "                best_acc_std = acc_std\n",
        "                best_step = current_step\n",
        "\n",
        "                best_state = {k: v.detach().cpu().clone() for k, v in eval_model.state_dict().items()}\n",
        "                best_ckpt = {\n",
        "                    \"step\": current_step,\n",
        "                    \"model_state\": best_state,\n",
        "                    \"best_acc_mean\": best_acc_mean,\n",
        "                    \"best_acc_std\": best_acc_std,\n",
        "                    \"best_step\": best_step,\n",
        "                    \"best_metric\": best_metric,\n",
        "                    \"loss_history\": loss_history,\n",
        "                    \"acc_history\": acc_history,\n",
        "                    \"config\": cfg,\n",
        "                }\n",
        "                atomic_save(best_ckpt, BEST_FILE)\n",
        "                print(f\" NEW BEST! step={best_step} acc={best_acc_mean:.4f}{best_acc_std:.4f} metric={best_metric:.4f}\")\n",
        "\n",
        "            # Plot\n",
        "            clear_output(wait=True)\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "            ax1.plot(loss_history, alpha=0.6)\n",
        "            if len(loss_history) > 500:\n",
        "                ma = np.convolve(loss_history, np.ones(500)/500, mode=\"valid\")\n",
        "                ax1.plot(np.arange(499, len(loss_history)), ma, linewidth=2)\n",
        "\n",
        "            ax1.set_title(f\"Loss | step={current_step}/{cfg['TARGET_STEPS']} | loss={loss_last:.4f} | K={k_at(max(0, current_step-1))}\")\n",
        "            ax1.set_yscale(\"log\")\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "\n",
        "            xs = [h[\"step\"] for h in acc_history]\n",
        "            ys = [h[\"acc_step1_mean\"] for h in acc_history]\n",
        "            ss = [h[\"acc_step1_std\"] for h in acc_history]\n",
        "            ax2.plot(xs, ys, \"o-\", linewidth=2, label=\"acc_step1\")\n",
        "            ax2.fill_between(xs, np.array(ys)-np.array(ss), np.array(ys)+np.array(ss), alpha=0.2)\n",
        "\n",
        "            ax2.set_ylim([0, 1])\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "            ax2.legend(loc=\"lower right\")\n",
        "\n",
        "            last = acc_history[-1]\n",
        "            ax2.set_title(\n",
        "                f\"acc={last['acc_step1_mean']:.4f}{last['acc_step1_std']:.4f} | 0={last['alpha0_mean']:.3f}\\n\"\n",
        "                f\"BEST={best_acc_mean:.4f}{best_acc_std:.4f} @ {best_step}\"\n",
        "            )\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            print(f\" step {current_step}/{cfg['TARGET_STEPS']} | K={last.get('k', 1)}\")\n",
        "            print(f\"   acc={last['acc_step1_mean']:.4f}{last['acc_step1_std']:.4f} | metric={last['metric']:.4f}\")\n",
        "            print(f\"   BEST: {best_acc_mean:.4f}{best_acc_std:.4f} @ {best_step}\")\n",
        "\n",
        "        dt = time.time() - t0\n",
        "        sps = cfg[\"BURST\"] / max(dt, 1e-9)\n",
        "        if current_step % cfg[\"EVAL_EVERY\"] != 0:\n",
        "            print(f\"step {current_step}/{cfg['TARGET_STEPS']} | K={k_at(max(0, current_step-1))} | {sps:.1f} steps/s\")\n",
        "\n",
        "    print(\"\\n Training complete!\")\n",
        "    print(f\"Final: {CKPT_FILE}\")\n",
        "    print(f\"Best:  {BEST_FILE}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# CONFIG\n",
        "# ============================================================\n",
        "CFG = {\n",
        "    \"EXPERIMENT_DIR\": \"/content/drive/My Drive/GradientShortCircuit_Experiments\",\n",
        "    \"RUN_TAG\": \"GSC_37dim_96hidden_K1_g2proxy_200k\",\n",
        "\n",
        "    \"TARGET_STEPS\": 2500_000,\n",
        "\n",
        "    \"BATCH\": 1024,\n",
        "    \"PROBE_LR\": 0.02,\n",
        "    \"BURST\": 200,\n",
        "\n",
        "    \"JERK_DECAY_RATE\": 0.0002,\n",
        "    \"MIN_JERK_SCALE\": 1e-4,\n",
        "    \"JERK_EVAL_SCALE\": 1e-4,\n",
        "\n",
        "    \"HUTCHINSON_SAMPLES\": 3,\n",
        "\n",
        "    \"DELTAL_SCALE\": 2.0,\n",
        "    \"DELTAL2_SCALE\": 5.0,\n",
        "\n",
        "    \"ALPHA_HEAD_MAX\": 0.5,\n",
        "    \"ALPHA_APPLY_MAX\": 1.5,\n",
        "    \"MULTS\": [1.8, 1.6, 2.0, 2.2],\n",
        "\n",
        "    \"USE_EMA\": True,\n",
        "    \"EMA_DECAY\": 0.9995,\n",
        "\n",
        "    \"LR\": 5e-4,\n",
        "\n",
        "    \"EVAL_EVERY\": 5000,\n",
        "    \"EVAL_TRIALS\": 10,\n",
        "}\n",
        "\n",
        "# GO\n",
        "train_fine_grained(CFG)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P3vJVLhw1gxf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4y3UOPs9S9r",
        "outputId": "24b3b97e-1c9d-45aa-f304-f7a4d025ff62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Running on cuda\n",
            "Mounted at /content/drive\n",
            " Pre-loading MNIST to GPU VRAM...\n",
            " Loaded hypertuner from GSC_Gold_OneStep.pth using model_state\n",
            "\n",
            "TEST A1: 10-TRIAL ZERO SHOT (ONE-SHOT)\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Trial |    Base |   Step1 |     Gain |     a0\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "1     |  11.83% |  96.10% |  +84.27% |  0.893\n",
            "2     |   8.56% |  96.12% |  +87.56% |  0.893\n",
            "3     |   4.77% |  96.09% |  +91.32% |  0.894\n",
            "4     |   8.69% |  96.12% |  +87.43% |  0.894\n",
            "5     |  11.97% |  96.07% |  +84.10% |  0.894\n",
            "6     |   6.52% |  96.28% |  +89.76% |  0.894\n",
            "7     |  10.33% |  95.88% |  +85.55% |  0.893\n",
            "8     |   9.91% |  96.19% |  +86.28% |  0.894\n",
            "9     |   6.84% |  96.07% |  +89.23% |  0.894\n",
            "10    |   7.22% |  96.22% |  +89.00% |  0.894\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            " AVERAGE Step1: 96.11% (0.10%) | Gain: +87.45%\n",
            "Min: 95.88% | Max: 96.28%\n",
            "\n",
            "========================================================================================================================\n",
            "TEST A2: 10-TRIAL ZERO SHOT (Support=1024)  Base vs Step1 vs Step2\n",
            "========================================================================================================================\n",
            "Trial |    Base |   Step1 |   Step2 |    (2-1) |     a0 |     a1\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "1     |   8.45% |  95.78% |  78.28% |   -17.50% |  0.894 |  0.900\n",
            "2     |   7.46% |  96.13% |  81.49% |   -14.64% |  0.894 |  0.900\n",
            "3     |   4.83% |  96.13% |  73.54% |   -22.59% |  0.894 |  0.900\n",
            "4     |  12.03% |  96.00% |  74.12% |   -21.88% |  0.894 |  0.901\n",
            "5     |   8.29% |  96.02% |  74.33% |   -21.69% |  0.894 |  0.899\n",
            "6     |  12.12% |  96.08% |  68.59% |   -27.49% |  0.895 |  0.902\n",
            "7     |  12.96% |  95.94% |  77.21% |   -18.73% |  0.894 |  0.901\n",
            "8     |  14.31% |  96.18% |  72.86% |   -23.32% |  0.894 |  0.900\n",
            "9     |   9.08% |  96.08% |  73.19% |   -22.89% |  0.896 |  0.901\n",
            "10    |  13.64% |  95.87% |  81.04% |   -14.83% |  0.894 |  0.900\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "MEAN  |  10.32% |  96.02% |  75.46% |   -20.56%\n",
            "STD   |   2.96% |   0.12% |   3.80%\n",
            "========================================================================================================================\n",
            "\n",
            "========================================================================================================================\n",
            " TEST B (GATED): SERIAL REFINEMENT  accept only if support CE improves\n",
            "========================================================================================================================\n",
            "Loop  0:    5.22%  (random init)\n",
            "Loop  1:   96.09%  (step1) | CE_support=0.1060 | a0=0.894 | dL1=-5.145e-01 dL2=-2.861e-01 cos=+0.943\n",
            "Loop  2:   96.05%   accepted | mult=0.125 | =0.113 | CE_support=0.1039\n",
            "Loop  3:   96.05%   rejected | CE_support=0.1039\n",
            "Loop  4:   96.05%   rejected | CE_support=0.1039\n",
            "Loop  5:   96.05%   rejected | CE_support=0.1039\n",
            "Loop  6:   96.05%   rejected | CE_support=0.1039\n",
            "Loop  7:   96.05%   rejected | CE_support=0.1039\n",
            "Loop  8:   96.05%   rejected | CE_support=0.1039\n",
            "Loop  9:   96.05%   rejected | CE_support=0.1039\n",
            "Loop 10:   96.05%   rejected | CE_support=0.1039\n",
            "Loop 11:   96.05%   rejected | CE_support=0.1039\n",
            "========================================================================================================================\n",
            " Done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from google.colab import drive\n",
        "\n",
        "# ============================================================\n",
        "# GSC EVAL SUITE  96-HIDDEN + 37-DIM FEATURES\n",
        "#\n",
        "# Compatible with training run:\n",
        "#   RUN_TAG = \"GSC_37dim_96hidden_K1_g2proxy_200k\"\n",
        "#\n",
        "# What it supports:\n",
        "# - TEST A1: 10-trial one-shot   (Base vs Step1)\n",
        "# - TEST A2: 10-trial two-step   (Base vs Step1 vs Step2, (2-1))\n",
        "# - TEST B : Serial refinement with gating + backtracking (support CE improves)\n",
        "#\n",
        "# Key details:\n",
        "# - Student: 784  96  10\n",
        "# - Hypertuner: 37-dim features (33 baseline + 4 fine-grained)\n",
        "# - Hessian proxy: g^2 (not Hutchinson)\n",
        "# - Probe returns: g1, accel, jerk, h_diag(g^2), dL1, dL2_raw, cos(g1,g2)\n",
        "# ============================================================\n",
        "\n",
        "torch.set_grad_enabled(True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\" Running on {device}\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# -----------------------------\n",
        "# SETTINGS\n",
        "# -----------------------------\n",
        "SUPPORT_BATCH = 1024\n",
        "PROBE_LR = 0.02\n",
        "\n",
        "TRIALS_ONESHOT = 10\n",
        "TRIALS_TWOSTEP = 10\n",
        "\n",
        "# Serial refinement (Test B)\n",
        "REFINE_LOOPS = 10\n",
        "BACKTRACK_MULTS = [1.0, 0.5, 0.25, 0.125]\n",
        "\n",
        "# Apply policy\n",
        "ALPHA_HEAD_MAX = 0.5\n",
        "APPLY_MAX = 1.5\n",
        "W0, B0, W1, B1 = 1.8, 1.6, 2.0, 2.2\n",
        "\n",
        "def mult_for_param(i: int) -> float:\n",
        "    return [W0, B0, W1, B1][i]\n",
        "\n",
        "# Probe settings\n",
        "DELTAL_SCALE = 2.0\n",
        "DELTAL2_SCALE = 5.0\n",
        "JERK_EVAL_SCALE = 1e-4\n",
        "\n",
        "# -----------------------------\n",
        "# DATA\n",
        "# -----------------------------\n",
        "class FastMNISTLoader:\n",
        "    def __init__(self, device):\n",
        "        print(\" Pre-loading MNIST to GPU VRAM...\")\n",
        "        self.device = device\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "        train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "        test_data  = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "        self.train_x = train_data.data.view(-1, 784).float().to(device) / 255.0\n",
        "        self.train_y = train_data.targets.to(device)\n",
        "        self.test_x  = test_data.data.view(-1, 784).float().to(device) / 255.0\n",
        "        self.test_y  = test_data.targets.to(device)\n",
        "\n",
        "        self.train_x = (self.train_x - 0.1307) / 0.3081\n",
        "        self.test_x  = (self.test_x  - 0.1307) / 0.3081\n",
        "\n",
        "        self.num_train = self.train_x.shape[0]\n",
        "\n",
        "    def sample_train(self, batch_size):\n",
        "        idx = torch.randint(0, self.num_train, (batch_size,), device=self.device)\n",
        "        return self.train_x[idx], self.train_y[idx]\n",
        "\n",
        "    def get_full_test(self):\n",
        "        return self.test_x, self.test_y\n",
        "\n",
        "# -----------------------------\n",
        "# STUDENT (96-hidden)\n",
        "# -----------------------------\n",
        "class MNISTStudent(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(784, 96),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(96, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def functional_forward(self, x, params):\n",
        "        x = F.linear(x, params[0], params[1])\n",
        "        x = F.relu(x)\n",
        "        x = F.linear(x, params[2], params[3])\n",
        "        return x\n",
        "\n",
        "def reset_student(net):\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "def soft_squash(x, eps=1e-8):\n",
        "    s = x.detach().abs().median().clamp(min=eps)\n",
        "    return torch.tanh(x / (3.0 * s))\n",
        "\n",
        "def signed_log1p(x):\n",
        "    return torch.sign(x) * torch.log1p(x.abs())\n",
        "\n",
        "# -----------------------------\n",
        "# PROBE (g^2 proxy for Hessian)\n",
        "# -----------------------------\n",
        "def compute_probe_and_curv(student, params, x_support, y_support,\n",
        "                          probe_lr=0.02, jerk_scale=1.0):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      g1, accel, jerk, h_diag(g^2), deltaL1_scalar, deltaL2_raw_scalar, cos_scalar\n",
        "    \"\"\"\n",
        "    p0 = [p.detach().clone().requires_grad_(True) for p in params]\n",
        "\n",
        "    logits1 = student.functional_forward(x_support, p0)\n",
        "    loss1 = F.cross_entropy(logits1, y_support)\n",
        "\n",
        "    grads1 = torch.autograd.grad(loss1, p0, create_graph=False)\n",
        "    g1 = [g.detach() for g in grads1]\n",
        "\n",
        "    # Simple Hessian proxy: g^2\n",
        "    h_diag = [g.pow(2) for g in g1]\n",
        "\n",
        "    # Step1 probe -> g2 + loss2\n",
        "    p1 = [(pp - probe_lr * gg).detach().requires_grad_(True) for pp, gg in zip(p0, g1)]\n",
        "    logits2 = student.functional_forward(x_support, p1)\n",
        "    loss2 = F.cross_entropy(logits2, y_support)\n",
        "\n",
        "    deltaL1 = float((loss2 - loss1).detach().item())\n",
        "\n",
        "    grads2 = torch.autograd.grad(loss2, p1, create_graph=False)\n",
        "    g2 = [g.detach() for g in grads2]\n",
        "    accel = [g2i - g1i for g1i, g2i in zip(g1, g2)]\n",
        "\n",
        "    # cos(g1,g2)\n",
        "    eps = 1e-12\n",
        "    dot = torch.zeros((), device=g1[0].device)\n",
        "    n1  = torch.zeros((), device=g1[0].device)\n",
        "    n2  = torch.zeros((), device=g1[0].device)\n",
        "    for a, b in zip(g1, g2):\n",
        "        aa = a.reshape(-1)\n",
        "        bb = b.reshape(-1)\n",
        "        dot = dot + (aa * bb).sum()\n",
        "        n1  = n1  + (aa * aa).sum()\n",
        "        n2  = n2  + (bb * bb).sum()\n",
        "    cosv = float((dot / (torch.sqrt(n1).clamp_min(eps) * torch.sqrt(n2).clamp_min(eps))).detach().item())\n",
        "\n",
        "    # Step2 probe -> g3 + loss3\n",
        "    p2 = [(pp - probe_lr * gg).detach().requires_grad_(True) for pp, gg in zip(p1, g2)]\n",
        "    logits3 = student.functional_forward(x_support, p2)\n",
        "    loss3 = F.cross_entropy(logits3, y_support)\n",
        "\n",
        "    deltaL2_raw = float((loss3 - loss2).detach().item())\n",
        "\n",
        "    grads3 = torch.autograd.grad(loss3, p2, create_graph=False)\n",
        "    g3 = [g.detach() for g in grads3]\n",
        "\n",
        "    jerk = [jerk_scale * (g3i - 2.0*g2i + g1i) for g1i, g2i, g3i in zip(g1, g2, g3)]\n",
        "    return g1, accel, jerk, h_diag, deltaL1, deltaL2_raw, cosv\n",
        "\n",
        "# -----------------------------\n",
        "# HYPERTUNER (37-dim)\n",
        "# -----------------------------\n",
        "class FineGrainedHypertuner37(nn.Module):\n",
        "    def __init__(self, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.layer_embeddings = nn.Embedding(num_layers, 10)\n",
        "\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(37, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        self.alpha_head = nn.Sequential(\n",
        "            nn.Linear(37, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            self.alpha_head[-1].bias.fill_(0.0)\n",
        "\n",
        "    def forward(self, weight, grad_g1, grad_accel, grad_jerk, h_diag, layer_idx,\n",
        "                support_ce_scalar, deltaL1_scalar, deltaL2_scalar, cos_scalar):\n",
        "        batch_size = weight.numel()\n",
        "        w_flat = weight.reshape(-1, 1)\n",
        "\n",
        "        g_flat = soft_squash(grad_g1).reshape(-1, 1)\n",
        "        a_flat = soft_squash(grad_accel).reshape(-1, 1)\n",
        "        j_flat = soft_squash(grad_jerk).reshape(-1, 1)\n",
        "        h_feat = soft_squash(signed_log1p(h_diag)).reshape(-1, 1)\n",
        "\n",
        "        dL1_feat = torch.tanh(torch.full(\n",
        "            (batch_size, 1),\n",
        "            float(deltaL1_scalar) * DELTAL_SCALE,\n",
        "            device=weight.device,\n",
        "            dtype=w_flat.dtype\n",
        "        ))\n",
        "        dL2_feat = torch.tanh(torch.full(\n",
        "            (batch_size, 1),\n",
        "            float(deltaL2_scalar) * DELTAL2_SCALE,\n",
        "            device=weight.device,\n",
        "            dtype=w_flat.dtype\n",
        "        ))\n",
        "\n",
        "        cos_feat = torch.full((batch_size, 1), float(cos_scalar), device=weight.device, dtype=w_flat.dtype)\n",
        "        ce_feat  = torch.full((batch_size, 1), float(support_ce_scalar), device=weight.device, dtype=w_flat.dtype)\n",
        "\n",
        "        # magnitude scalars\n",
        "        log_mag_g1    = float(torch.log1p(grad_g1.detach().abs().median()).item())\n",
        "        log_mag_accel = float(torch.log1p(grad_accel.detach().abs().median()).item())\n",
        "        log_mag_jerk  = float(torch.log1p(grad_jerk.detach().abs().median()).item())\n",
        "        log_mag_h     = float(torch.log1p(h_diag.detach().abs().median()).item())\n",
        "        mag_features = torch.tensor([log_mag_g1, log_mag_accel, log_mag_jerk, log_mag_h],\n",
        "                                    device=weight.device, dtype=w_flat.dtype).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Fine-grained features\n",
        "        current_center = float(weight.mean().item())\n",
        "        dist_current_center = torch.log1p((w_flat - current_center).abs())\n",
        "        center_drift = torch.log1p(torch.abs(torch.tensor(current_center, device=weight.device)))\n",
        "        center_drift_feat = torch.full((batch_size, 1), float(center_drift.item()),\n",
        "                                      device=weight.device, dtype=w_flat.dtype)\n",
        "        grad_mag_raw = torch.log1p(grad_g1.abs()).reshape(-1, 1)\n",
        "        h_to_g_ratio = soft_squash(h_diag.abs() / (grad_g1.abs() + 1e-8)).reshape(-1, 1)\n",
        "\n",
        "        if weight.dim() > 1:\n",
        "            rows, cols = weight.shape\n",
        "            r_idx = torch.arange(rows, device=weight.device)\n",
        "            c_idx = torch.arange(cols, device=weight.device)\n",
        "            r = r_idx.repeat_interleave(cols).float().reshape(-1, 1) / rows\n",
        "            c = c_idx.repeat(rows).float().reshape(-1, 1) / cols\n",
        "\n",
        "            center_row, center_col = (rows - 1) / 2.0, (cols - 1) / 2.0\n",
        "            rr = r_idx.repeat_interleave(cols).float()\n",
        "            cc = c_idx.repeat(rows).float()\n",
        "            dist = torch.sqrt((rr - center_row) ** 2 + (cc - center_col) ** 2)\n",
        "            max_dist = torch.sqrt(torch.tensor(center_row**2 + center_col**2,\n",
        "                                               device=weight.device, dtype=dist.dtype)).clamp(min=1e-8)\n",
        "            dist_center = (dist / max_dist).reshape(-1, 1)\n",
        "\n",
        "            fan_in = max(int(cols), 1)\n",
        "            sigma = torch.sqrt(torch.tensor(2.0 / fan_in, device=weight.device, dtype=w_flat.dtype)).clamp(min=1e-8)\n",
        "            dist_origin = torch.log1p(w_flat.abs() / sigma)\n",
        "\n",
        "            row_norms = grad_g1.norm(dim=1, keepdim=True)\n",
        "            col_norms = grad_g1.norm(dim=0, keepdim=True).t()\n",
        "            row_feat = soft_squash(row_norms.repeat_interleave(cols, dim=0))\n",
        "            col_feat = soft_squash(col_norms.repeat(rows, 1))\n",
        "\n",
        "            g_mean = g_flat.mean().expand(batch_size, 1)\n",
        "            g_std  = g_flat.std().expand(batch_size, 1)\n",
        "\n",
        "            t_in  = torch.full((batch_size, 1), float(np.log1p(cols)), device=weight.device, dtype=w_flat.dtype)\n",
        "            t_out = torch.full((batch_size, 1), float(np.log1p(rows)), device=weight.device, dtype=w_flat.dtype)\n",
        "\n",
        "        else:\n",
        "            rows = weight.shape[0]\n",
        "            r_idx = torch.arange(rows, device=weight.device)\n",
        "            r = r_idx.float().reshape(-1, 1) / rows\n",
        "            c = torch.zeros_like(r)\n",
        "\n",
        "            center = (rows - 1) / 2.0\n",
        "            dist = (r_idx.float() - center).abs()\n",
        "            max_dist = torch.tensor(center, device=weight.device, dtype=dist.dtype).clamp(min=1e-8)\n",
        "            dist_center = (dist / max_dist).reshape(-1, 1)\n",
        "\n",
        "            fan_in = 784 if layer_idx == 0 else 96  # 96 for 96-hidden student\n",
        "            sigma = torch.sqrt(torch.tensor(2.0 / fan_in, device=weight.device, dtype=w_flat.dtype)).clamp(min=1e-8)\n",
        "            dist_origin = torch.log1p(w_flat.abs() / sigma)\n",
        "\n",
        "            row_feat = soft_squash(grad_g1.abs().reshape(-1, 1))\n",
        "            col_feat = torch.zeros_like(row_feat)\n",
        "\n",
        "            g_mean = g_flat.mean().expand(batch_size, 1)\n",
        "            g_std  = g_flat.std().expand(batch_size, 1)\n",
        "\n",
        "            t_in  = torch.zeros((batch_size, 1), device=weight.device, dtype=w_flat.dtype)\n",
        "            t_out = torch.full((batch_size, 1), float(np.log1p(rows)), device=weight.device, dtype=w_flat.dtype)\n",
        "\n",
        "        l_emb = self.layer_embeddings(torch.tensor([layer_idx], device=weight.device)).expand(batch_size, -1)\n",
        "\n",
        "        inputs = torch.cat([\n",
        "            w_flat, g_flat, a_flat, j_flat,      # 4\n",
        "            l_emb,                               # 10\n",
        "            g_mean, g_std,                       # 2\n",
        "            t_in, t_out,                         # 2\n",
        "            r, c,                                # 2\n",
        "            dist_center, dist_origin,            # 2\n",
        "            row_feat, col_feat,                  # 2\n",
        "            h_feat,                              # 1\n",
        "            dL1_feat, dL2_feat,                  # 2\n",
        "            cos_feat, ce_feat,                   # 2\n",
        "            mag_features,                        # 4\n",
        "            # Fine-grained (4)\n",
        "            dist_current_center,                 # 1\n",
        "            center_drift_feat,                   # 1\n",
        "            grad_mag_raw,                        # 1\n",
        "            h_to_g_ratio,                        # 1\n",
        "        ], dim=1)  # 37\n",
        "\n",
        "        delta = 0.5 * torch.tanh(self.predictor(inputs))\n",
        "        p_full = (w_flat + delta).view_as(weight)\n",
        "\n",
        "        a0 = ALPHA_HEAD_MAX * torch.sigmoid(self.alpha_head(inputs))\n",
        "        return p_full, a0.view_as(weight)\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def acc_on_full_test(student, params, x_test, y_test):\n",
        "    return (student.functional_forward(x_test, params).argmax(1) == y_test).float().mean().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def support_ce(student, x_s, y_s, params):\n",
        "    return float(F.cross_entropy(student.functional_forward(x_s, params), y_s).item())\n",
        "\n",
        "def apply_step(params_in, p_full_list, a_list, mult_scale=1.0):\n",
        "    out = []\n",
        "    alpha_means = []\n",
        "    for i, (p, pf, a) in enumerate(zip(params_in, p_full_list, a_list)):\n",
        "        au = torch.clamp(a * mult_for_param(i) * mult_scale, 0.0, APPLY_MAX)\n",
        "        out.append(p + au * (pf - p))\n",
        "        alpha_means.append(float(au.mean().item()))\n",
        "    return out, float(np.mean(alpha_means))\n",
        "\n",
        "def step_once(ht, student, params_in, x_s, y_s):\n",
        "    \"\"\"\n",
        "    One application (Step1-style): uses probe on params_in and returns new params.\n",
        "    \"\"\"\n",
        "    ce = support_ce(student, x_s, y_s, params_in)\n",
        "\n",
        "    with torch.enable_grad():\n",
        "        g1, accel, jerk, h0, dL1, dL2, cosv = compute_probe_and_curv(\n",
        "            student, params_in, x_s, y_s,\n",
        "            probe_lr=PROBE_LR,\n",
        "            jerk_scale=JERK_EVAL_SCALE\n",
        "        )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        p_full, a_list = [], []\n",
        "        for i, (p, g1i, ai, ji, hi) in enumerate(zip(params_in, g1, accel, jerk, h0)):\n",
        "            pf, a = ht(p, g1i, ai, ji, hi, i // 2, ce, dL1, dL2, cosv)\n",
        "            p_full.append(pf)\n",
        "            a_list.append(a)\n",
        "        out, a_mean = apply_step(params_in, p_full, a_list, mult_scale=1.0)\n",
        "\n",
        "    return out, ce, dL1, dL2, cosv, a_mean\n",
        "\n",
        "def step1_and_step2(ht, student, params0, x_s, y_s):\n",
        "    # Step1\n",
        "    step1, ce0, dL1_0, dL2_0, cos0, a0_mean = step_once(ht, student, params0, x_s, y_s)\n",
        "    # Step2 (run the same operator again starting from step1)\n",
        "    step2, ce1, dL1_1, dL2_1, cos1, a1_mean = step_once(ht, student, step1, x_s, y_s)\n",
        "    return step1, step2, ce0, ce1, a0_mean, a1_mean, (dL1_0, dL2_0, cos0), (dL1_1, dL2_1, cos1)\n",
        "\n",
        "# -----------------------------\n",
        "# LOAD HT\n",
        "# -----------------------------\n",
        "def load_hypertuner_from_ckpt(ckpt_path: str, use_ema: bool = True):\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    if isinstance(ckpt, dict):\n",
        "        if use_ema and (\"ema_state\" in ckpt) and (ckpt[\"ema_state\"] is not None):\n",
        "            state = ckpt[\"ema_state\"]\n",
        "            which = \"ema_state\"\n",
        "        elif \"model_state\" in ckpt:\n",
        "            state = ckpt[\"model_state\"]\n",
        "            which = \"model_state\"\n",
        "        else:\n",
        "            state = ckpt\n",
        "            which = \"ckpt(dict)\"\n",
        "    else:\n",
        "        state = ckpt\n",
        "        which = \"ckpt(raw)\"\n",
        "    ht = FineGrainedHypertuner37(num_layers=2).to(device)\n",
        "    ht.load_state_dict(state, strict=False)\n",
        "    ht.eval()\n",
        "    print(f\" Loaded hypertuner from {os.path.basename(ckpt_path)} using {which}\")\n",
        "    return ht\n",
        "\n",
        "# ============================================================\n",
        "# MAIN\n",
        "# ============================================================\n",
        "def main(CHECKPOINT_TO_TEST: str, experiment_dir: str, USE_EMA_WEIGHTS: bool = True):\n",
        "    fast = FastMNISTLoader(device)\n",
        "    student = MNISTStudent().to(device)\n",
        "    x_test, y_test = fast.get_full_test()\n",
        "\n",
        "    if not os.path.exists(CHECKPOINT_TO_TEST):\n",
        "        print(\" Checkpoint not found:\", CHECKPOINT_TO_TEST)\n",
        "        print(\"\\nAvailable .pth files in experiment_dir:\")\n",
        "        if os.path.exists(experiment_dir):\n",
        "            for f in sorted(os.listdir(experiment_dir)):\n",
        "                if f.endswith(\".pth\"):\n",
        "                    print(\" -\", f)\n",
        "        raise FileNotFoundError(CHECKPOINT_TO_TEST)\n",
        "\n",
        "    ht = load_hypertuner_from_ckpt(CHECKPOINT_TO_TEST, use_ema=USE_EMA_WEIGHTS)\n",
        "\n",
        "    # ============================================================\n",
        "    # TEST A1: ONE-SHOT (Base vs Step1)\n",
        "    # ============================================================\n",
        "    print(\"\\nTEST A1: 10-TRIAL ZERO SHOT (ONE-SHOT)\")\n",
        "    print(\"-\"*110)\n",
        "    print(f\"{'Trial':<5} | {'Base':>7} | {'Step1':>7} | {'Gain':>8} | {'a0':>6}\")\n",
        "    print(\"-\"*110)\n",
        "\n",
        "    base_accs = []\n",
        "    s1_accs = []\n",
        "\n",
        "    for t in range(TRIALS_ONESHOT):\n",
        "        reset_student(student)\n",
        "        student.eval()\n",
        "        params0 = [p.detach().clone() for p in student.parameters()]\n",
        "        x_s, y_s = fast.sample_train(SUPPORT_BATCH)\n",
        "\n",
        "        base = acc_on_full_test(student, params0, x_test, y_test)\n",
        "\n",
        "        step1, _, _, _, a0m, _, _, _ = step1_and_step2(ht, student, params0, x_s, y_s)\n",
        "        tele = acc_on_full_test(student, step1, x_test, y_test)\n",
        "\n",
        "        base_accs.append(base)\n",
        "        s1_accs.append(tele)\n",
        "\n",
        "        print(f\"{t+1:<5} | {base*100:6.2f}% | {tele*100:6.2f}% | {(tele-base)*100:+7.2f}% | {a0m:6.3f}\")\n",
        "\n",
        "    print(\"-\"*110)\n",
        "    print(f\" AVERAGE Step1: {np.mean(s1_accs)*100:.2f}% ({np.std(s1_accs)*100:.2f}%) | Gain: {(np.mean(s1_accs)-np.mean(base_accs))*100:+.2f}%\")\n",
        "    print(f\"Min: {np.min(s1_accs)*100:.2f}% | Max: {np.max(s1_accs)*100:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    # ============================================================\n",
        "    # TEST A2: TWO-STEP (Base vs Step1 vs Step2)\n",
        "    # ============================================================\n",
        "    print(\"=\"*120)\n",
        "    print(\"TEST A2: 10-TRIAL ZERO SHOT (Support=1024)  Base vs Step1 vs Step2\")\n",
        "    print(\"=\"*120)\n",
        "    print(f\"{'Trial':<5} | {'Base':>7} | {'Step1':>7} | {'Step2':>7} | {'(2-1)':>9} | {'a0':>6} | {'a1':>6}\")\n",
        "    print(\"-\"*120)\n",
        "\n",
        "    base2 = []\n",
        "    s1 = []\n",
        "    s2 = []\n",
        "\n",
        "    for t in range(TRIALS_TWOSTEP):\n",
        "        reset_student(student)\n",
        "        student.eval()\n",
        "        params0 = [p.detach().clone() for p in student.parameters()]\n",
        "        x_s, y_s = fast.sample_train(SUPPORT_BATCH)\n",
        "\n",
        "        b = acc_on_full_test(student, params0, x_test, y_test)\n",
        "        step1, step2, _, _, a0m, a1m, _, _ = step1_and_step2(ht, student, params0, x_s, y_s)\n",
        "        a1 = acc_on_full_test(student, step1, x_test, y_test)\n",
        "        a2 = acc_on_full_test(student, step2, x_test, y_test)\n",
        "\n",
        "        base2.append(b); s1.append(a1); s2.append(a2)\n",
        "        print(f\"{t+1:<5} | {b*100:6.2f}% | {a1*100:6.2f}% | {a2*100:6.2f}% | {(a2-a1)*100:+8.2f}% | {a0m:6.3f} | {a1m:6.3f}\")\n",
        "\n",
        "    print(\"-\"*120)\n",
        "    print(f\"MEAN  | {np.mean(base2)*100:6.2f}% | {np.mean(s1)*100:6.2f}% | {np.mean(s2)*100:6.2f}% | {(np.mean(s2)-np.mean(s1))*100:+8.2f}%\")\n",
        "    print(f\"STD   | {np.std(base2)*100:6.2f}% | {np.std(s1)*100:6.2f}% | {np.std(s2)*100:6.2f}%\")\n",
        "    print(\"=\"*120)\n",
        "    print()\n",
        "\n",
        "    # ============================================================\n",
        "    # TEST B: SERIAL REFINEMENT (GATED)\n",
        "    # accept only if support CE improves; backtrack multipliers allowed\n",
        "    # ============================================================\n",
        "    print(\"=\"*120)\n",
        "    print(\" TEST B (GATED): SERIAL REFINEMENT  accept only if support CE improves\")\n",
        "    print(\"=\"*120)\n",
        "\n",
        "    reset_student(student)\n",
        "    student.eval()\n",
        "    params0 = [p.detach().clone() for p in student.parameters()]\n",
        "    x_s, y_s = fast.sample_train(SUPPORT_BATCH)\n",
        "\n",
        "    acc0 = acc_on_full_test(student, params0, x_test, y_test)\n",
        "    print(f\"Loop  0: {acc0*100:7.2f}%  (random init)\")\n",
        "\n",
        "    # Step1\n",
        "    step1, _, _, ce1, a0m, _, scal0, _ = step1_and_step2(ht, student, params0, x_s, y_s)\n",
        "    acc1 = acc_on_full_test(student, step1, x_test, y_test)\n",
        "    dL1_0, dL2_0, cos0 = scal0\n",
        "    print(f\"Loop  1: {acc1*100:7.2f}%  (step1) | CE_support={ce1:.4f} | a0={a0m:.3f} | dL1={dL1_0:+.3e} dL2={dL2_0:+.3e} cos={cos0:+.3f}\")\n",
        "\n",
        "    current_params = step1\n",
        "    current_ce = ce1\n",
        "\n",
        "    for k in range(2, REFINE_LOOPS + 2):\n",
        "        # Build a step proposal from current_params (like Step2), then backtrack.\n",
        "        ce_in = support_ce(student, x_s, y_s, current_params)\n",
        "\n",
        "        with torch.enable_grad():\n",
        "            g1b, accel_b, jerk_b, h1, dL1b, dL2b, cosb = compute_probe_and_curv(\n",
        "                student, current_params, x_s, y_s,\n",
        "                probe_lr=PROBE_LR,\n",
        "                jerk_scale=JERK_EVAL_SCALE\n",
        "            )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            p_full, a_list = [], []\n",
        "            for i, (p, g1i, ai, ji, hi) in enumerate(zip(current_params, g1b, accel_b, jerk_b, h1)):\n",
        "                pf, a = ht(p, g1i, ai, ji, hi, i // 2, ce_in, dL1b, dL2b, cosb)\n",
        "                p_full.append(pf); a_list.append(a)\n",
        "\n",
        "        best_candidate = None\n",
        "        best_ce = current_ce\n",
        "        best_mult = None\n",
        "        best_alpha_mean = None\n",
        "\n",
        "        for mult in BACKTRACK_MULTS:\n",
        "            with torch.no_grad():\n",
        "                cand_params, a_mean = apply_step(current_params, p_full, a_list, mult_scale=mult)\n",
        "                cand_ce = support_ce(student, x_s, y_s, cand_params)\n",
        "\n",
        "            if cand_ce < best_ce - 1e-12:\n",
        "                best_ce = cand_ce\n",
        "                best_candidate = cand_params\n",
        "                best_mult = mult\n",
        "                best_alpha_mean = a_mean\n",
        "\n",
        "        if best_candidate is None:\n",
        "            acc_now = acc_on_full_test(student, current_params, x_test, y_test)\n",
        "            print(f\"Loop {k:>2}: {acc_now*100:7.2f}%   rejected | CE_support={current_ce:.4f}\")\n",
        "            continue\n",
        "\n",
        "        current_params = best_candidate\n",
        "        current_ce = best_ce\n",
        "        acc_now = acc_on_full_test(student, current_params, x_test, y_test)\n",
        "        print(f\"Loop {k:>2}: {acc_now*100:7.2f}%   accepted | mult={best_mult:0.3f} | ={best_alpha_mean:.3f} | CE_support={current_ce:.4f}\")\n",
        "\n",
        "    print(\"=\"*120)\n",
        "    print(\" Done.\")\n",
        "\n",
        "# ============================================================\n",
        "# USER SETTINGS (edit this block only)\n",
        "# ============================================================\n",
        "experiment_dir = '/content/drive/My Drive/GradientShortCircuit_Experiments/gold_checkpoints'\n",
        "\n",
        "#  Set your checkpoint filename here\n",
        "CKPT_BASENAME = 'GSC_Gold_OneStep.pth'\n",
        "\n",
        "CHECKPOINT_TO_TEST = os.path.join(experiment_dir, CKPT_BASENAME)\n",
        "\n",
        "# Load EMA weights if available in ckpt (recommended)\n",
        "USE_EMA_WEIGHTS = True\n",
        "\n",
        "# GO\n",
        "main(CHECKPOINT_TO_TEST=CHECKPOINT_TO_TEST, experiment_dir=experiment_dir, USE_EMA_WEIGHTS=USE_EMA_WEIGHTS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from google.colab import drive\n",
        "\n",
        "# ============================================================\n",
        "# COMPREHENSIVE TEST SUITE\n",
        "#\n",
        "# Tests:\n",
        "#   1. 64-hidden student (smaller) on MNIST\n",
        "#   2. 128-hidden student (bigger) on MNIST\n",
        "#   3. 96-hidden student on Fashion-MNIST\n",
        "#   4. 96-hidden student on KMNIST\n",
        "#\n",
        "# Uses: Phase A checkpoint (96-hidden trained hypertuner)\n",
        "# Goal: Test generalization across architectures and datasets\n",
        "# ============================================================\n",
        "\n",
        "torch.set_grad_enabled(True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\" Running on {device}\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# -----------------------------\n",
        "# SETTINGS\n",
        "# -----------------------------\n",
        "SUPPORT_BATCH = 1024\n",
        "PROBE_LR = 0.02\n",
        "TRIALS = 10\n",
        "\n",
        "ALPHA_HEAD_MAX = 0.5\n",
        "APPLY_MAX = 1.5\n",
        "W0, B0, W1, B1 = 1.8, 1.6, 2.0, 2.2\n",
        "\n",
        "def mult_for_param(i: int) -> float:\n",
        "    return [W0, B0, W1, B1][i]\n",
        "\n",
        "DELTAL_SCALE = 2.0\n",
        "DELTAL2_SCALE = 5.0\n",
        "JERK_EVAL_SCALE = 1e-4\n",
        "\n",
        "# -----------------------------\n",
        "# DATA LOADERS\n",
        "# -----------------------------\n",
        "class FastMNISTLoader:\n",
        "    def __init__(self, device, dataset='MNIST'):\n",
        "        print(f\" Pre-loading {dataset} to GPU VRAM...\")\n",
        "        self.device = device\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "\n",
        "        if dataset == 'MNIST':\n",
        "            train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "            test_data  = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "        elif dataset == 'FashionMNIST':\n",
        "            train_data = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
        "            test_data  = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
        "        elif dataset == 'KMNIST':\n",
        "            train_data = datasets.KMNIST('./data', train=True, download=True, transform=transform)\n",
        "            test_data  = datasets.KMNIST('./data', train=False, download=True, transform=transform)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown dataset: {dataset}\")\n",
        "\n",
        "        self.train_x = train_data.data.view(-1, 784).float().to(device) / 255.0\n",
        "        self.train_y = train_data.targets.to(device)\n",
        "        self.test_x  = test_data.data.view(-1, 784).float().to(device) / 255.0\n",
        "        self.test_y  = test_data.targets.to(device)\n",
        "\n",
        "        self.train_x = (self.train_x - 0.1307) / 0.3081\n",
        "        self.test_x  = (self.test_x  - 0.1307) / 0.3081\n",
        "\n",
        "        self.num_train = self.train_x.shape[0]\n",
        "\n",
        "    def sample_train(self, batch_size):\n",
        "        idx = torch.randint(0, self.num_train, (batch_size,), device=self.device)\n",
        "        return self.train_x[idx], self.train_y[idx]\n",
        "\n",
        "    def get_full_test(self):\n",
        "        return self.test_x, self.test_y\n",
        "\n",
        "# -----------------------------\n",
        "# STUDENTS (Variable Hidden Size)\n",
        "# -----------------------------\n",
        "class MNISTStudent(nn.Module):\n",
        "    def __init__(self, hidden_size=96):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(784, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def functional_forward(self, x, params):\n",
        "        x = F.linear(x, params[0], params[1])\n",
        "        x = F.relu(x)\n",
        "        x = F.linear(x, params[2], params[3])\n",
        "        return x\n",
        "\n",
        "def reset_student(net):\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "def soft_squash(x, eps=1e-8):\n",
        "    s = x.detach().abs().median().clamp(min=eps)\n",
        "    return torch.tanh(x / (3.0 * s))\n",
        "\n",
        "def signed_log1p(x):\n",
        "    return torch.sign(x) * torch.log1p(x.abs())\n",
        "\n",
        "# -----------------------------\n",
        "# PROBE\n",
        "# -----------------------------\n",
        "def compute_probe_and_curv(student, params, x_support, y_support,\n",
        "                          probe_lr=0.02, jerk_scale=1.0):\n",
        "    p0 = [p.detach().clone().requires_grad_(True) for p in params]\n",
        "\n",
        "    logits1 = student.functional_forward(x_support, p0)\n",
        "    loss1 = F.cross_entropy(logits1, y_support)\n",
        "\n",
        "    grads1 = torch.autograd.grad(loss1, p0, create_graph=False)\n",
        "    g1 = [g.detach() for g in grads1]\n",
        "    h_diag = [g.pow(2) for g in g1]\n",
        "\n",
        "    p1 = [(pp - probe_lr * gg).detach().requires_grad_(True) for pp, gg in zip(p0, g1)]\n",
        "    logits2 = student.functional_forward(x_support, p1)\n",
        "    loss2 = F.cross_entropy(logits2, y_support)\n",
        "    deltaL1 = float((loss2 - loss1).detach().item())\n",
        "    grads2 = torch.autograd.grad(loss2, p1, create_graph=False)\n",
        "    g2 = [g.detach() for g in grads2]\n",
        "    accel = [g2i - g1i for g1i, g2i in zip(g1, g2)]\n",
        "\n",
        "    eps = 1e-12\n",
        "    dot = torch.zeros((), device=g1[0].device)\n",
        "    n1  = torch.zeros((), device=g1[0].device)\n",
        "    n2  = torch.zeros((), device=g1[0].device)\n",
        "    for a, b in zip(g1, g2):\n",
        "        aa = a.reshape(-1)\n",
        "        bb = b.reshape(-1)\n",
        "        dot = dot + (aa * bb).sum()\n",
        "        n1  = n1  + (aa * aa).sum()\n",
        "        n2  = n2  + (bb * bb).sum()\n",
        "    cosv = float((dot / (torch.sqrt(n1).clamp_min(eps) * torch.sqrt(n2).clamp_min(eps))).detach().item())\n",
        "\n",
        "    p2 = [(pp - probe_lr * gg).detach().requires_grad_(True) for pp, gg in zip(p1, g2)]\n",
        "    logits3 = student.functional_forward(x_support, p2)\n",
        "    loss3 = F.cross_entropy(logits3, y_support)\n",
        "    deltaL2_raw = float((loss3 - loss2).detach().item())\n",
        "    grads3 = torch.autograd.grad(loss3, p2, create_graph=False)\n",
        "    g3 = [g.detach() for g in grads3]\n",
        "\n",
        "    jerk = [jerk_scale * (g3i - 2.0*g2i + g1i) for g1i, g2i, g3i in zip(g1, g2, g3)]\n",
        "    return g1, accel, jerk, h_diag, deltaL1, deltaL2_raw, cosv\n",
        "\n",
        "# -----------------------------\n",
        "# HYPERTUNER (37-dim, trained on 96-hidden)\n",
        "# -----------------------------\n",
        "class FineGrainedHypertuner37(nn.Module):\n",
        "    def __init__(self, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.layer_embeddings = nn.Embedding(num_layers, 10)\n",
        "\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(37, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        self.alpha_head = nn.Sequential(\n",
        "            nn.Linear(37, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            self.alpha_head[-1].bias.fill_(0.0)\n",
        "\n",
        "    def forward(self, weight, grad_g1, grad_accel, grad_jerk, h_diag, layer_idx,\n",
        "                support_ce_scalar, deltaL1_scalar, deltaL2_scalar, cos_scalar,\n",
        "                student_hidden_size=96):  # Added parameter\n",
        "        batch_size = weight.numel()\n",
        "        w_flat = weight.reshape(-1, 1)\n",
        "\n",
        "        g_flat = soft_squash(grad_g1).reshape(-1, 1)\n",
        "        a_flat = soft_squash(grad_accel).reshape(-1, 1)\n",
        "        j_flat = soft_squash(grad_jerk).reshape(-1, 1)\n",
        "        h_feat = soft_squash(signed_log1p(h_diag)).reshape(-1, 1)\n",
        "\n",
        "        dL1_feat = torch.tanh(torch.full(\n",
        "            (batch_size, 1),\n",
        "            float(deltaL1_scalar) * DELTAL_SCALE,\n",
        "            device=weight.device,\n",
        "            dtype=w_flat.dtype\n",
        "        ))\n",
        "        dL2_feat = torch.tanh(torch.full(\n",
        "            (batch_size, 1),\n",
        "            float(deltaL2_scalar) * DELTAL2_SCALE,\n",
        "            device=weight.device,\n",
        "            dtype=w_flat.dtype\n",
        "        ))\n",
        "\n",
        "        cos_feat = torch.full((batch_size, 1), float(cos_scalar), device=weight.device, dtype=w_flat.dtype)\n",
        "        ce_feat  = torch.full((batch_size, 1), float(support_ce_scalar), device=weight.device, dtype=w_flat.dtype)\n",
        "\n",
        "        log_mag_g1    = float(torch.log1p(grad_g1.detach().abs().median()).item())\n",
        "        log_mag_accel = float(torch.log1p(grad_accel.detach().abs().median()).item())\n",
        "        log_mag_jerk  = float(torch.log1p(grad_jerk.detach().abs().median()).item())\n",
        "        log_mag_h     = float(torch.log1p(h_diag.detach().abs().median()).item())\n",
        "        mag_features = torch.tensor([log_mag_g1, log_mag_accel, log_mag_jerk, log_mag_h],\n",
        "                                    device=weight.device, dtype=w_flat.dtype).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        current_center = float(weight.mean().item())\n",
        "        dist_current_center = torch.log1p((w_flat - current_center).abs())\n",
        "        center_drift = torch.log1p(torch.abs(torch.tensor(current_center, device=weight.device)))\n",
        "        center_drift_feat = torch.full((batch_size, 1), float(center_drift.item()),\n",
        "                                      device=weight.device, dtype=w_flat.dtype)\n",
        "        grad_mag_raw = torch.log1p(grad_g1.abs()).reshape(-1, 1)\n",
        "        h_to_g_ratio = soft_squash(h_diag.abs() / (grad_g1.abs() + 1e-8)).reshape(-1, 1)\n",
        "\n",
        "        if weight.dim() > 1:\n",
        "            rows, cols = weight.shape\n",
        "            r_idx = torch.arange(rows, device=weight.device)\n",
        "            c_idx = torch.arange(cols, device=weight.device)\n",
        "            r = r_idx.repeat_interleave(cols).float().reshape(-1, 1) / rows\n",
        "            c = c_idx.repeat(rows).float().reshape(-1, 1) / cols\n",
        "\n",
        "            center_row, center_col = (rows - 1) / 2.0, (cols - 1) / 2.0\n",
        "            rr = r_idx.repeat_interleave(cols).float()\n",
        "            cc = c_idx.repeat(rows).float()\n",
        "            dist = torch.sqrt((rr - center_row) ** 2 + (cc - center_col) ** 2)\n",
        "            max_dist = torch.sqrt(torch.tensor(center_row**2 + center_col**2,\n",
        "                                               device=weight.device, dtype=dist.dtype)).clamp(min=1e-8)\n",
        "            dist_center = (dist / max_dist).reshape(-1, 1)\n",
        "\n",
        "            fan_in = max(int(cols), 1)\n",
        "            sigma = torch.sqrt(torch.tensor(2.0 / fan_in, device=weight.device, dtype=w_flat.dtype)).clamp(min=1e-8)\n",
        "            dist_origin = torch.log1p(w_flat.abs() / sigma)\n",
        "\n",
        "            row_norms = grad_g1.norm(dim=1, keepdim=True)\n",
        "            col_norms = grad_g1.norm(dim=0, keepdim=True).t()\n",
        "            row_feat = soft_squash(row_norms.repeat_interleave(cols, dim=0))\n",
        "            col_feat = soft_squash(col_norms.repeat(rows, 1))\n",
        "\n",
        "            g_mean = g_flat.mean().expand(batch_size, 1)\n",
        "            g_std  = g_flat.std().expand(batch_size, 1)\n",
        "\n",
        "            t_in  = torch.full((batch_size, 1), float(np.log1p(cols)), device=weight.device, dtype=w_flat.dtype)\n",
        "            t_out = torch.full((batch_size, 1), float(np.log1p(rows)), device=weight.device, dtype=w_flat.dtype)\n",
        "\n",
        "        else:\n",
        "            rows = weight.shape[0]\n",
        "            r_idx = torch.arange(rows, device=weight.device)\n",
        "            r = r_idx.float().reshape(-1, 1) / rows\n",
        "            c = torch.zeros_like(r)\n",
        "\n",
        "            center = (rows - 1) / 2.0\n",
        "            dist = (r_idx.float() - center).abs()\n",
        "            max_dist = torch.tensor(center, device=weight.device, dtype=dist.dtype).clamp(min=1e-8)\n",
        "            dist_center = (dist / max_dist).reshape(-1, 1)\n",
        "\n",
        "            # Use actual student hidden size for fan_in calculation\n",
        "            fan_in = 784 if layer_idx == 0 else student_hidden_size\n",
        "            sigma = torch.sqrt(torch.tensor(2.0 / fan_in, device=weight.device, dtype=w_flat.dtype)).clamp(min=1e-8)\n",
        "            dist_origin = torch.log1p(w_flat.abs() / sigma)\n",
        "\n",
        "            row_feat = soft_squash(grad_g1.abs().reshape(-1, 1))\n",
        "            col_feat = torch.zeros_like(row_feat)\n",
        "\n",
        "            g_mean = g_flat.mean().expand(batch_size, 1)\n",
        "            g_std  = g_flat.std().expand(batch_size, 1)\n",
        "\n",
        "            t_in  = torch.zeros((batch_size, 1), device=weight.device, dtype=w_flat.dtype)\n",
        "            t_out = torch.full((batch_size, 1), float(np.log1p(rows)), device=weight.device, dtype=w_flat.dtype)\n",
        "\n",
        "        l_emb = self.layer_embeddings(torch.tensor([layer_idx], device=weight.device)).expand(batch_size, -1)\n",
        "\n",
        "        inputs = torch.cat([\n",
        "            w_flat, g_flat, a_flat, j_flat, l_emb, g_mean, g_std, t_in, t_out, r, c,\n",
        "            dist_center, dist_origin, row_feat, col_feat, h_feat, dL1_feat, dL2_feat,\n",
        "            cos_feat, ce_feat, mag_features, dist_current_center, center_drift_feat,\n",
        "            grad_mag_raw, h_to_g_ratio,\n",
        "        ], dim=1)\n",
        "\n",
        "        delta = 0.5 * torch.tanh(self.predictor(inputs))\n",
        "        p_full = (w_flat + delta).view_as(weight)\n",
        "\n",
        "        a0 = ALPHA_HEAD_MAX * torch.sigmoid(self.alpha_head(inputs))\n",
        "        return p_full, a0.view_as(weight)\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def acc_on_full_test(student, params, x_test, y_test):\n",
        "    return (student.functional_forward(x_test, params).argmax(1) == y_test).float().mean().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def support_ce(student, x_s, y_s, params):\n",
        "    return float(F.cross_entropy(student.functional_forward(x_s, params), y_s).item())\n",
        "\n",
        "def apply_step(params_in, p_full_list, a_list, mult_scale=1.0):\n",
        "    out = []\n",
        "    alpha_means = []\n",
        "    for i, (p, pf, a) in enumerate(zip(params_in, p_full_list, a_list)):\n",
        "        au = torch.clamp(a * mult_for_param(i) * mult_scale, 0.0, APPLY_MAX)\n",
        "        out.append(p + au * (pf - p))\n",
        "        alpha_means.append(float(au.mean().item()))\n",
        "    return out, float(np.mean(alpha_means))\n",
        "\n",
        "def step_once(ht, student, params_in, x_s, y_s, student_hidden_size=96):\n",
        "    ce = support_ce(student, x_s, y_s, params_in)\n",
        "\n",
        "    with torch.enable_grad():\n",
        "        g1, accel, jerk, h0, dL1, dL2, cosv = compute_probe_and_curv(\n",
        "            student, params_in, x_s, y_s,\n",
        "            probe_lr=PROBE_LR,\n",
        "            jerk_scale=JERK_EVAL_SCALE\n",
        "        )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        p_full, a_list = [], []\n",
        "        for i, (p, g1i, ai, ji, hi) in enumerate(zip(params_in, g1, accel, jerk, h0)):\n",
        "            pf, a = ht(p, g1i, ai, ji, hi, i // 2, ce, dL1, dL2, cosv,\n",
        "                      student_hidden_size=student_hidden_size)\n",
        "            p_full.append(pf)\n",
        "            a_list.append(a)\n",
        "        out, a_mean = apply_step(params_in, p_full, a_list, mult_scale=1.0)\n",
        "\n",
        "    return out, ce, dL1, dL2, cosv, a_mean\n",
        "\n",
        "# -----------------------------\n",
        "# LOAD HT\n",
        "# -----------------------------\n",
        "def load_hypertuner_from_ckpt(ckpt_path: str, use_ema: bool = True):\n",
        "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
        "    if isinstance(ckpt, dict):\n",
        "        if use_ema and (\"ema_state\" in ckpt) and (ckpt[\"ema_state\"] is not None):\n",
        "            state = ckpt[\"ema_state\"]\n",
        "            which = \"ema_state\"\n",
        "        elif \"model_state\" in ckpt:\n",
        "            state = ckpt[\"model_state\"]\n",
        "            which = \"model_state\"\n",
        "        else:\n",
        "            state = ckpt\n",
        "            which = \"ckpt(dict)\"\n",
        "    else:\n",
        "        state = ckpt\n",
        "        which = \"ckpt(raw)\"\n",
        "    ht = FineGrainedHypertuner37(num_layers=2).to(device)\n",
        "    ht.load_state_dict(state, strict=False)\n",
        "    ht.eval()\n",
        "    print(f\" Loaded hypertuner from {os.path.basename(ckpt_path)} using {which}\")\n",
        "    return ht\n",
        "\n",
        "# ============================================================\n",
        "# TEST FUNCTIONS\n",
        "# ============================================================\n",
        "def test_on_architecture(ht, hidden_size, dataset='MNIST', trials=10):\n",
        "    \"\"\"Test hypertuner on different architecture size\"\"\"\n",
        "    print(\"\\n\" + \"=\"*120)\n",
        "    print(f\" TEST: {hidden_size}-hidden student on {dataset}\")\n",
        "    print(\"=\"*120)\n",
        "\n",
        "    fast_loader = FastMNISTLoader(device, dataset=dataset)\n",
        "    student = MNISTStudent(hidden_size=hidden_size).to(device)\n",
        "    x_test, y_test = fast_loader.get_full_test()\n",
        "\n",
        "    print(f\"{'Trial':<5} | {'Base':>7} | {'Step1':>7} | {'Gain':>8} | {'a0':>6}\")\n",
        "    print(\"-\"*110)\n",
        "\n",
        "    base_accs = []\n",
        "    s1_accs = []\n",
        "\n",
        "    for t in range(trials):\n",
        "        reset_student(student)\n",
        "        student.eval()\n",
        "        params0 = [p.detach().clone() for p in student.parameters()]\n",
        "        x_s, y_s = fast_loader.sample_train(SUPPORT_BATCH)\n",
        "\n",
        "        base = acc_on_full_test(student, params0, x_test, y_test)\n",
        "\n",
        "        step1, _, _, _, _, a0m = step_once(ht, student, params0, x_s, y_s,\n",
        "                                          student_hidden_size=hidden_size)\n",
        "        tele = acc_on_full_test(student, step1, x_test, y_test)\n",
        "\n",
        "        base_accs.append(base)\n",
        "        s1_accs.append(tele)\n",
        "\n",
        "        print(f\"{t+1:<5} | {base*100:6.2f}% | {tele*100:6.2f}% | {(tele-base)*100:+7.2f}% | {a0m:6.3f}\")\n",
        "\n",
        "    print(\"-\"*110)\n",
        "    print(f\" AVERAGE Step1: {np.mean(s1_accs)*100:.2f}% ({np.std(s1_accs)*100:.2f}%) | Gain: {(np.mean(s1_accs)-np.mean(base_accs))*100:+.2f}%\")\n",
        "    print(f\"Min: {np.min(s1_accs)*100:.2f}% | Max: {np.max(s1_accs)*100:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    return {\n",
        "        'architecture': f'{hidden_size}-hidden',\n",
        "        'dataset': dataset,\n",
        "        'mean': np.mean(s1_accs),\n",
        "        'std': np.std(s1_accs),\n",
        "        'min': np.min(s1_accs),\n",
        "        'max': np.max(s1_accs),\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# MAIN\n",
        "# ============================================================\n",
        "def main():\n",
        "    experiment_dir = '/content/drive/My Drive/GradientShortCircuit_Experiments'\n",
        "    CKPT_BASENAME = 'GSC_37dim_96hidden_K1_g2proxy_200k_BEST.pth'\n",
        "    CHECKPOINT_TO_TEST = os.path.join(experiment_dir, CKPT_BASENAME)\n",
        "\n",
        "    print(\"=\"*120)\n",
        "    print(\" COMPREHENSIVE GENERALIZATION TEST SUITE\")\n",
        "    print(\"=\"*120)\n",
        "    print(\"Testing Phase A checkpoint across:\")\n",
        "    print(\"  - Different architectures (64, 96, 128 hidden)\")\n",
        "    print(\"  - Different datasets (MNIST, Fashion-MNIST, KMNIST)\")\n",
        "    print(\"=\"*120)\n",
        "\n",
        "    ht = load_hypertuner_from_ckpt(CHECKPOINT_TO_TEST, use_ema=True)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Test 1: 64-hidden on MNIST\n",
        "    print(\"\\n TEST 1/5: Smaller student (64-hidden) on MNIST\")\n",
        "    results.append(test_on_architecture(ht, 64, 'MNIST', trials=TRIALS))\n",
        "\n",
        "    # Test 2: 96-hidden on MNIST (baseline, already done but for completeness)\n",
        "    print(\"\\n TEST 2/5: Trained size (96-hidden) on MNIST [BASELINE]\")\n",
        "    results.append(test_on_architecture(ht, 96, 'MNIST', trials=TRIALS))\n",
        "\n",
        "    # Test 3: 128-hidden on MNIST\n",
        "    print(\"\\n TEST 3/5: Larger student (128-hidden) on MNIST\")\n",
        "    results.append(test_on_architecture(ht, 128, 'MNIST', trials=TRIALS))\n",
        "\n",
        "    # Test 4: 96-hidden on Fashion-MNIST\n",
        "    print(\"\\n TEST 4/5: 96-hidden on Fashion-MNIST (domain transfer)\")\n",
        "    results.append(test_on_architecture(ht, 96, 'FashionMNIST', trials=TRIALS))\n",
        "\n",
        "    # Test 5: 96-hidden on KMNIST\n",
        "    print(\"\\n TEST 5/5: 96-hidden on KMNIST (domain transfer)\")\n",
        "    results.append(test_on_architecture(ht, 96, 'KMNIST', trials=TRIALS))\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*120)\n",
        "    print(\" SUMMARY OF ALL TESTS\")\n",
        "    print(\"=\"*120)\n",
        "    print(f\"{'Test':<40} | {'Mean':>8} | {'Std':>6} | {'Min':>8} | {'Max':>8}\")\n",
        "    print(\"-\"*120)\n",
        "    for r in results:\n",
        "        test_name = f\"{r['architecture']} on {r['dataset']}\"\n",
        "        print(f\"{test_name:<40} | {r['mean']*100:7.2f}% | {r['std']*100:5.2f}% | {r['min']*100:7.2f}% | {r['max']*100:7.2f}%\")\n",
        "    print(\"=\"*120)\n",
        "\n",
        "    # Analysis\n",
        "    print(\"\\n ANALYSIS:\")\n",
        "    mnist_results = [r for r in results if r['dataset'] == 'MNIST']\n",
        "    print(f\"\\n Architecture Generalization (MNIST):\")\n",
        "    for r in mnist_results:\n",
        "        print(f\"  {r['architecture']}: {r['mean']*100:.2f}%\")\n",
        "\n",
        "    print(f\"\\n Domain Transfer (96-hidden):\")\n",
        "    transfer_results = [r for r in results if r['architecture'] == '96-hidden']\n",
        "    for r in transfer_results:\n",
        "        print(f\"  {r['dataset']}: {r['mean']*100:.2f}%\")\n",
        "\n",
        "    print(\"\\n Testing complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdz1rhxv8VvC",
        "outputId": "6d68b920-5d31-4e53-fbe9-37e65427bccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Running on cuda\n",
            "Mounted at /content/drive\n",
            "========================================================================================================================\n",
            " COMPREHENSIVE GENERALIZATION TEST SUITE\n",
            "========================================================================================================================\n",
            "Testing Phase A checkpoint across:\n",
            "  - Different architectures (64, 96, 128 hidden)\n",
            "  - Different datasets (MNIST, Fashion-MNIST, KMNIST)\n",
            "========================================================================================================================\n",
            " Loaded hypertuner from GSC_37dim_96hidden_K1_g2proxy_200k_BEST.pth using model_state\n",
            "\n",
            " TEST 1/5: Smaller student (64-hidden) on MNIST\n",
            "\n",
            "========================================================================================================================\n",
            " TEST: 64-hidden student on MNIST\n",
            "========================================================================================================================\n",
            " Pre-loading MNIST to GPU VRAM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9.91M/9.91M [00:01<00:00, 4.97MB/s]\n",
            "100%|| 28.9k/28.9k [00:00<00:00, 129kB/s]\n",
            "100%|| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n",
            "100%|| 4.54k/4.54k [00:00<00:00, 10.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial |    Base |   Step1 |     Gain |     a0\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "1     |   8.26% |  27.97% |  +19.71% |  0.891\n",
            "2     |  12.37% |  29.91% |  +17.54% |  0.892\n",
            "3     |   9.34% |  27.38% |  +18.04% |  0.892\n",
            "4     |   9.93% |  28.16% |  +18.23% |  0.892\n",
            "5     |  11.99% |  29.50% |  +17.51% |  0.892\n",
            "6     |   8.67% |  32.48% |  +23.81% |  0.892\n",
            "7     |  16.10% |  27.48% |  +11.38% |  0.891\n",
            "8     |   9.59% |  29.63% |  +20.04% |  0.890\n",
            "9     |   8.84% |  31.77% |  +22.93% |  0.892\n",
            "10    |  12.30% |  28.98% |  +16.68% |  0.892\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            " AVERAGE Step1: 29.33% (1.64%) | Gain: +18.59%\n",
            "Min: 27.38% | Max: 32.48%\n",
            "\n",
            "\n",
            " TEST 2/5: Trained size (96-hidden) on MNIST [BASELINE]\n",
            "\n",
            "========================================================================================================================\n",
            " TEST: 96-hidden student on MNIST\n",
            "========================================================================================================================\n",
            " Pre-loading MNIST to GPU VRAM...\n",
            "Trial |    Base |   Step1 |     Gain |     a0\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "1     |   9.11% |  96.10% |  +86.99% |  0.894\n",
            "2     |  10.21% |  96.03% |  +85.82% |  0.893\n",
            "3     |  12.15% |  95.59% |  +83.44% |  0.894\n",
            "4     |  13.20% |  96.09% |  +82.89% |  0.894\n",
            "5     |   9.76% |  95.97% |  +86.21% |  0.894\n",
            "6     |  13.02% |  96.09% |  +83.07% |  0.894\n",
            "7     |  13.74% |  95.94% |  +82.20% |  0.895\n",
            "8     |   7.56% |  96.08% |  +88.52% |  0.894\n",
            "9     |  16.11% |  95.97% |  +79.86% |  0.894\n",
            "10    |   7.67% |  96.13% |  +88.46% |  0.894\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            " AVERAGE Step1: 96.00% (0.15%) | Gain: +84.75%\n",
            "Min: 95.59% | Max: 96.13%\n",
            "\n",
            "\n",
            " TEST 3/5: Larger student (128-hidden) on MNIST\n",
            "\n",
            "========================================================================================================================\n",
            " TEST: 128-hidden student on MNIST\n",
            "========================================================================================================================\n",
            " Pre-loading MNIST to GPU VRAM...\n",
            "Trial |    Base |   Step1 |     Gain |     a0\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "1     |  10.56% |  41.66% |  +31.10% |  0.896\n",
            "2     |  13.63% |  39.04% |  +25.41% |  0.896\n",
            "3     |   8.81% |  41.57% |  +32.76% |  0.895\n",
            "4     |  12.60% |  37.90% |  +25.30% |  0.894\n",
            "5     |   9.67% |  40.50% |  +30.83% |  0.894\n",
            "6     |  10.33% |  38.95% |  +28.62% |  0.895\n",
            "7     |  11.75% |  39.58% |  +27.83% |  0.895\n",
            "8     |   9.02% |  41.53% |  +32.51% |  0.896\n",
            "9     |  12.04% |  41.48% |  +29.44% |  0.895\n",
            "10    |   5.93% |  40.28% |  +34.35% |  0.895\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            " AVERAGE Step1: 40.25% (1.27%) | Gain: +29.81%\n",
            "Min: 37.90% | Max: 41.66%\n",
            "\n",
            "\n",
            " TEST 4/5: 96-hidden on Fashion-MNIST (domain transfer)\n",
            "\n",
            "========================================================================================================================\n",
            " TEST: 96-hidden student on FashionMNIST\n",
            "========================================================================================================================\n",
            " Pre-loading FashionMNIST to GPU VRAM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 26.4M/26.4M [00:03<00:00, 8.15MB/s]\n",
            "100%|| 29.5k/29.5k [00:00<00:00, 173kB/s]\n",
            "100%|| 4.42M/4.42M [00:01<00:00, 3.23MB/s]\n",
            "100%|| 5.15k/5.15k [00:00<00:00, 13.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial |    Base |   Step1 |     Gain |     a0\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "1     |   8.77% |   2.40% |   -6.37% |  0.892\n",
            "2     |  10.37% |   1.83% |   -8.54% |  0.893\n",
            "3     |  12.57% |   2.42% |  -10.15% |  0.890\n",
            "4     |  10.16% |   2.02% |   -8.14% |  0.888\n",
            "5     |  11.61% |   1.92% |   -9.69% |  0.885\n",
            "6     |   3.83% |   2.98% |   -0.85% |  0.892\n",
            "7     |  10.13% |   1.89% |   -8.24% |  0.885\n",
            "8     |   4.26% |   2.10% |   -2.16% |  0.888\n",
            "9     |   8.58% |   1.82% |   -6.76% |  0.893\n",
            "10    |  10.62% |   4.38% |   -6.24% |  0.894\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            " AVERAGE Step1: 2.38% (0.75%) | Gain: -6.71%\n",
            "Min: 1.82% | Max: 4.38%\n",
            "\n",
            "\n",
            " TEST 5/5: 96-hidden on KMNIST (domain transfer)\n",
            "\n",
            "========================================================================================================================\n",
            " TEST: 96-hidden student on KMNIST\n",
            "========================================================================================================================\n",
            " Pre-loading KMNIST to GPU VRAM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 18.2M/18.2M [00:09<00:00, 1.95MB/s]\n",
            "100%|| 29.5k/29.5k [00:00<00:00, 382kB/s]\n",
            "100%|| 3.04M/3.04M [00:01<00:00, 2.38MB/s]\n",
            "100%|| 5.12k/5.12k [00:00<00:00, 14.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial |    Base |   Step1 |     Gain |     a0\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "1     |   9.24% |  13.66% |   +4.42% |  0.892\n",
            "2     |   8.91% |  14.81% |   +5.90% |  0.891\n",
            "3     |  10.27% |  14.56% |   +4.29% |  0.892\n",
            "4     |   6.53% |  14.03% |   +7.50% |  0.892\n",
            "5     |   9.43% |  14.43% |   +5.00% |  0.891\n",
            "6     |   7.87% |  13.75% |   +5.88% |  0.892\n",
            "7     |  10.92% |  14.00% |   +3.08% |  0.891\n",
            "8     |  11.53% |  14.03% |   +2.50% |  0.891\n",
            "9     |   6.04% |  14.04% |   +8.00% |  0.894\n",
            "10    |   5.87% |  13.36% |   +7.49% |  0.894\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            " AVERAGE Step1: 14.07% (0.41%) | Gain: +5.41%\n",
            "Min: 13.36% | Max: 14.81%\n",
            "\n",
            "\n",
            "========================================================================================================================\n",
            " SUMMARY OF ALL TESTS\n",
            "========================================================================================================================\n",
            "Test                                     |     Mean |    Std |      Min |      Max\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "64-hidden on MNIST                       |   29.33% |  1.64% |   27.38% |   32.48%\n",
            "96-hidden on MNIST                       |   96.00% |  0.15% |   95.59% |   96.13%\n",
            "128-hidden on MNIST                      |   40.25% |  1.27% |   37.90% |   41.66%\n",
            "96-hidden on FashionMNIST                |    2.38% |  0.75% |    1.82% |    4.38%\n",
            "96-hidden on KMNIST                      |   14.07% |  0.41% |   13.36% |   14.81%\n",
            "========================================================================================================================\n",
            "\n",
            " ANALYSIS:\n",
            "\n",
            " Architecture Generalization (MNIST):\n",
            "  64-hidden: 29.33%\n",
            "  96-hidden: 96.00%\n",
            "  128-hidden: 40.25%\n",
            "\n",
            " Domain Transfer (96-hidden):\n",
            "  MNIST: 96.00%\n",
            "  FashionMNIST: 2.38%\n",
            "  KMNIST: 14.07%\n",
            "\n",
            " Testing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "GSC one-hop refinement characterization (FIXED).\n",
        "\n",
        "This script:\n",
        "  - pretrains a fresh student into target accuracy bands (approx),\n",
        "  - applies EXACTLY ONE GSC hop (Hop1),\n",
        "  - reports Before -> After -> Gain (paper-friendly),\n",
        "  - flags whether the start was IN-BAND.\n",
        "\n",
        "This version is fixed to avoid autograd graph issues and to MATCH the\n",
        "37-dim hypertuner architecture/feature construction used in your current\n",
        "evaluation scripts (GSC_Gold_OneStep.pth).\n",
        "\n",
        "Usage (Colab):\n",
        "  !python GSC_refinement_onehop.py\n",
        "\n",
        "Edit CONFIG below as needed.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -----------------------------\n",
        "# DEVICE + REPRO\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEED = 1337\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "experiment_dir = \"/content/drive/My Drive/GradientShortCircuit_Experiments\"\n",
        "CHECKPOINT_TO_TEST = \"GSC_Gold_OneStep.pth\"  # expected in experiment_dir/gold_checkpoints/\n",
        "USE_EMA_WEIGHTS = True\n",
        "\n",
        "STUDENT_HIDDEN = 96\n",
        "SUPPORT_BATCH = 1024\n",
        "\n",
        "# Pretrain-to-band\n",
        "TARGET_CENTERS = [0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90]\n",
        "TARGET_TOL = 0.03\n",
        "TRIALS_PER_TARGET = 10\n",
        "\n",
        "PRETRAIN_LR = 0.10\n",
        "PRETRAIN_MAX_STEPS = 1000\n",
        "PRETRAIN_EVAL_EVERY = 5\n",
        "MAX_RESTARTS = 8  # per target band\n",
        "\n",
        "# Probe + features\n",
        "PROBE_LR = 0.02\n",
        "JERK_EVAL_SCALE = 1e-4\n",
        "\n",
        "# Apply policy (match your eval scripts)\n",
        "ALPHA_HEAD_MAX = 0.5\n",
        "APPLY_MAX = 1.5\n",
        "W0, B0, W1, B1 = 1.8, 1.6, 2.0, 2.2\n",
        "DELTAL_SCALE = 2.0\n",
        "DELTAL2_SCALE = 5.0\n",
        "\n",
        "def mult_for_param(i: int) -> float:\n",
        "    return [W0, B0, W1, B1][i]\n",
        "\n",
        "# -----------------------------\n",
        "# Auto-mount Drive in Colab\n",
        "# -----------------------------\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    from google.colab import drive  # type: ignore\n",
        "    if experiment_dir.startswith(\"/content/drive\"):\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "        print(\"Mounted at /content/drive\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# -----------------------------\n",
        "# DATA\n",
        "# -----------------------------\n",
        "class FastMNISTLoader:\n",
        "    \"\"\"Preload MNIST train/test to GPU using raw tensors (fast).\"\"\"\n",
        "    def __init__(self, device: torch.device):\n",
        "        from torchvision import datasets, transforms\n",
        "\n",
        "        tfm = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "        root = \"./data\"\n",
        "        train = datasets.MNIST(root, train=True, download=True, transform=tfm)\n",
        "        test  = datasets.MNIST(root, train=False, download=True, transform=tfm)\n",
        "\n",
        "        # Use underlying tensors for speed; apply normalization manually\n",
        "        xtr = train.data.view(-1, 784).float().to(device) / 255.0\n",
        "        ytr = train.targets.to(device)\n",
        "        xte = test.data.view(-1, 784).float().to(device) / 255.0\n",
        "        yte = test.targets.to(device)\n",
        "\n",
        "        xtr = (xtr - 0.1307) / 0.3081\n",
        "        xte = (xte - 0.1307) / 0.3081\n",
        "\n",
        "        self.x_train, self.y_train = xtr, ytr\n",
        "        self.x_test, self.y_test = xte, yte\n",
        "        self.num_train = xtr.shape[0]\n",
        "        print(\" Pre-loaded MNIST to GPU\")\n",
        "\n",
        "    def sample_train(self, n: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        idx = torch.randint(0, self.num_train, (n,), device=self.x_train.device)\n",
        "        return self.x_train[idx], self.y_train[idx]\n",
        "\n",
        "    def get_full_test(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        return self.x_test, self.y_test\n",
        "\n",
        "# -----------------------------\n",
        "# STUDENT (functional)\n",
        "# -----------------------------\n",
        "class StudentNet(nn.Module):\n",
        "    def __init__(self, hidden: int):\n",
        "        super().__init__()\n",
        "        self.hidden = hidden\n",
        "        self.fc1 = nn.Linear(784, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "    def functional_forward(self, x: torch.Tensor, params: List[torch.Tensor]) -> torch.Tensor:\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.linear(x, params[0], params[1])\n",
        "        x = F.relu(x)\n",
        "        x = F.linear(x, params[2], params[3])\n",
        "        return x\n",
        "\n",
        "def reset_student(student: StudentNet):\n",
        "    for m in student.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "def init_params(student: StudentNet) -> List[torch.Tensor]:\n",
        "    return [student.fc1.weight.detach().clone(),\n",
        "            student.fc1.bias.detach().clone(),\n",
        "            student.fc2.weight.detach().clone(),\n",
        "            student.fc2.bias.detach().clone()]\n",
        "\n",
        "# -----------------------------\n",
        "# Feature helpers (match your eval)\n",
        "# -----------------------------\n",
        "def soft_squash(x, eps=1e-8):\n",
        "    s = x.detach().abs().median().clamp(min=eps)\n",
        "    return torch.tanh(x / (3.0 * s))\n",
        "\n",
        "def signed_log1p(x):\n",
        "    return torch.sign(x) * torch.log1p(x.abs())\n",
        "\n",
        "@torch.no_grad()\n",
        "def acc_on_full_test(student: StudentNet, params: List[torch.Tensor], x_test: torch.Tensor, y_test: torch.Tensor) -> float:\n",
        "    logits = student.functional_forward(x_test, params)\n",
        "    return (logits.argmax(1) == y_test).float().mean().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def support_ce(student: StudentNet, params: List[torch.Tensor], x_s: torch.Tensor, y_s: torch.Tensor) -> float:\n",
        "    logits = student.functional_forward(x_s, params)\n",
        "    return float(F.cross_entropy(logits, y_s).item())\n",
        "\n",
        "# -----------------------------\n",
        "# PROBE (g^2 Hessian proxy)\n",
        "# -----------------------------\n",
        "def compute_probe_and_curv(student: StudentNet, params: List[torch.Tensor], x: torch.Tensor, y: torch.Tensor,\n",
        "                           probe_lr: float, jerk_scale: float):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      g1, accel, jerk, h_diag(g^2), dL1(loss2-loss1), dL2_raw(loss3-loss2), cos(g1,g2)\n",
        "    \"\"\"\n",
        "    p0 = [p.detach().clone().requires_grad_(True) for p in params]\n",
        "\n",
        "    logits1 = student.functional_forward(x, p0)\n",
        "    loss1 = F.cross_entropy(logits1, y)\n",
        "    grads1 = torch.autograd.grad(loss1, p0, create_graph=False)\n",
        "    g1 = [g.detach() for g in grads1]\n",
        "\n",
        "    h_diag = [g.pow(2) for g in g1]\n",
        "\n",
        "    # probe step 1\n",
        "    p1 = [(pp - probe_lr * gg).detach().requires_grad_(True) for pp, gg in zip(p0, g1)]\n",
        "    logits2 = student.functional_forward(x, p1)\n",
        "    loss2 = F.cross_entropy(logits2, y)\n",
        "    dL1 = float((loss2 - loss1).detach().item())\n",
        "    grads2 = torch.autograd.grad(loss2, p1, create_graph=False)\n",
        "    g2 = [g.detach() for g in grads2]\n",
        "    accel = [g2i - g1i for g1i, g2i in zip(g1, g2)]\n",
        "\n",
        "    # cos(g1,g2)\n",
        "    eps = 1e-12\n",
        "    dot = torch.zeros((), device=x.device)\n",
        "    n1  = torch.zeros((), device=x.device)\n",
        "    n2  = torch.zeros((), device=x.device)\n",
        "    for a, b in zip(g1, g2):\n",
        "        aa = a.reshape(-1)\n",
        "        bb = b.reshape(-1)\n",
        "        dot = dot + (aa * bb).sum()\n",
        "        n1  = n1  + (aa * aa).sum()\n",
        "        n2  = n2  + (bb * bb).sum()\n",
        "    cosv = float((dot / (torch.sqrt(n1).clamp_min(eps) * torch.sqrt(n2).clamp_min(eps))).detach().item())\n",
        "\n",
        "    # probe step 2\n",
        "    p2 = [(pp - probe_lr * gg).detach().requires_grad_(True) for pp, gg in zip(p1, g2)]\n",
        "    logits3 = student.functional_forward(x, p2)\n",
        "    loss3 = F.cross_entropy(logits3, y)\n",
        "    dL2_raw = float((loss3 - loss2).detach().item())\n",
        "    grads3 = torch.autograd.grad(loss3, p2, create_graph=False)\n",
        "    g3 = [g.detach() for g in grads3]\n",
        "\n",
        "    jerk = [float(jerk_scale) * (g3i - 2.0*g2i + g1i) for g1i, g2i, g3i in zip(g1, g2, g3)]\n",
        "    return g1, accel, jerk, h_diag, dL1, dL2_raw, cosv\n",
        "\n",
        "# -----------------------------\n",
        "# HYPERTUNER (MATCHED)\n",
        "# -----------------------------\n",
        "class FineGrainedHypertuner37(nn.Module):\n",
        "    def __init__(self, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.layer_embeddings = nn.Embedding(num_layers, 10)\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(37, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        self.alpha_head = nn.Sequential(\n",
        "            nn.Linear(37, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            self.alpha_head[-1].bias.fill_(0.0)\n",
        "\n",
        "    def forward(self, weight, grad_g1, grad_accel, grad_jerk, h_diag, layer_idx,\n",
        "                support_ce_scalar, deltaL1_scalar, deltaL2_scalar, cos_scalar):\n",
        "        batch_size = weight.numel()\n",
        "        w_flat = weight.reshape(-1, 1)\n",
        "\n",
        "        g_flat = soft_squash(grad_g1).reshape(-1, 1)\n",
        "        a_flat = soft_squash(grad_accel).reshape(-1, 1)\n",
        "        j_flat = soft_squash(grad_jerk).reshape(-1, 1)\n",
        "        h_feat = soft_squash(signed_log1p(h_diag)).reshape(-1, 1)\n",
        "\n",
        "        dL1_feat = torch.tanh(torch.full((batch_size, 1), float(deltaL1_scalar) * DELTAL_SCALE,\n",
        "                                         device=weight.device, dtype=w_flat.dtype))\n",
        "        dL2_feat = torch.tanh(torch.full((batch_size, 1), float(deltaL2_scalar) * DELTAL2_SCALE,\n",
        "                                         device=weight.device, dtype=w_flat.dtype))\n",
        "        cos_feat = torch.full((batch_size, 1), float(cos_scalar), device=weight.device, dtype=w_flat.dtype)\n",
        "        ce_feat  = torch.full((batch_size, 1), float(support_ce_scalar), device=weight.device, dtype=w_flat.dtype)\n",
        "\n",
        "        log_mag_g1    = float(torch.log1p(grad_g1.detach().abs().median()).item())\n",
        "        log_mag_accel = float(torch.log1p(grad_accel.detach().abs().median()).item())\n",
        "        log_mag_jerk  = float(torch.log1p(grad_jerk.detach().abs().median()).item())\n",
        "        log_mag_h     = float(torch.log1p(h_diag.detach().abs().median()).item())\n",
        "        mag_features = torch.tensor([log_mag_g1, log_mag_accel, log_mag_jerk, log_mag_h],\n",
        "                                    device=weight.device, dtype=w_flat.dtype).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Fine-grained features\n",
        "        current_center = float(weight.mean().item())\n",
        "        dist_current_center = torch.log1p((w_flat - current_center).abs())\n",
        "        center_drift = torch.log1p(torch.abs(torch.tensor(current_center, device=weight.device)))\n",
        "        center_drift_feat = torch.full((batch_size, 1), float(center_drift.item()),\n",
        "                                      device=weight.device, dtype=w_flat.dtype)\n",
        "        grad_mag_raw = torch.log1p(grad_g1.abs()).reshape(-1, 1)\n",
        "        h_to_g_ratio = soft_squash(h_diag.abs() / (grad_g1.abs() + 1e-8)).reshape(-1, 1)\n",
        "\n",
        "        # Geometry features\n",
        "        if weight.dim() > 1:\n",
        "            rows, cols = weight.shape\n",
        "            r_idx = torch.arange(rows, device=weight.device)\n",
        "            c_idx = torch.arange(cols, device=weight.device)\n",
        "            r = r_idx.repeat_interleave(cols).float().reshape(-1, 1) / rows\n",
        "            c = c_idx.repeat(rows).float().reshape(-1, 1) / cols\n",
        "\n",
        "            center_row, center_col = (rows - 1) / 2.0, (cols - 1) / 2.0\n",
        "            rr = r_idx.repeat_interleave(cols).float()\n",
        "            cc = c_idx.repeat(rows).float()\n",
        "            dist = torch.sqrt((rr - center_row) ** 2 + (cc - center_col) ** 2)\n",
        "            max_dist = torch.sqrt(torch.tensor(center_row**2 + center_col**2,\n",
        "                                               device=weight.device, dtype=dist.dtype)).clamp(min=1e-8)\n",
        "            dist_center = (dist / max_dist).reshape(-1, 1)\n",
        "\n",
        "            fan_in = max(int(cols), 1)\n",
        "            sigma = torch.sqrt(torch.tensor(2.0 / fan_in, device=weight.device, dtype=w_flat.dtype)).clamp(min=1e-8)\n",
        "            dist_origin = torch.log1p(w_flat.abs() / sigma)\n",
        "\n",
        "            row_norms = grad_g1.norm(dim=1, keepdim=True)\n",
        "            col_norms = grad_g1.norm(dim=0, keepdim=True).t()\n",
        "            row_feat = soft_squash(row_norms.repeat_interleave(cols, dim=0))\n",
        "            col_feat = soft_squash(col_norms.repeat(rows, 1))\n",
        "\n",
        "            g_mean = g_flat.mean().expand(batch_size, 1)\n",
        "            g_std  = g_flat.std().expand(batch_size, 1)\n",
        "\n",
        "            t_in  = torch.full((batch_size, 1), float(np.log1p(cols)), device=weight.device, dtype=w_flat.dtype)\n",
        "            t_out = torch.full((batch_size, 1), float(np.log1p(rows)), device=weight.device, dtype=w_flat.dtype)\n",
        "        else:\n",
        "            rows = weight.shape[0]\n",
        "            r_idx = torch.arange(rows, device=weight.device)\n",
        "            r = r_idx.float().reshape(-1, 1) / rows\n",
        "            c = torch.zeros_like(r)\n",
        "\n",
        "            center = (rows - 1) / 2.0\n",
        "            dist = (r_idx.float() - center).abs()\n",
        "            max_dist = torch.tensor(center, device=weight.device, dtype=dist.dtype).clamp(min=1e-8)\n",
        "            dist_center = (dist / max_dist).reshape(-1, 1)\n",
        "\n",
        "            fan_in = 784 if layer_idx == 0 else STUDENT_HIDDEN\n",
        "            sigma = torch.sqrt(torch.tensor(2.0 / fan_in, device=weight.device, dtype=w_flat.dtype)).clamp(min=1e-8)\n",
        "            dist_origin = torch.log1p(w_flat.abs() / sigma)\n",
        "\n",
        "            row_feat = soft_squash(grad_g1.abs().reshape(-1, 1))\n",
        "            col_feat = torch.zeros_like(row_feat)\n",
        "\n",
        "            g_mean = g_flat.mean().expand(batch_size, 1)\n",
        "            g_std  = g_flat.std().expand(batch_size, 1)\n",
        "\n",
        "            t_in  = torch.zeros((batch_size, 1), device=weight.device, dtype=w_flat.dtype)\n",
        "            t_out = torch.full((batch_size, 1), float(np.log1p(rows)), device=weight.device, dtype=w_flat.dtype)\n",
        "\n",
        "        l_emb = self.layer_embeddings(torch.tensor([layer_idx], device=weight.device)).expand(batch_size, -1)\n",
        "\n",
        "        inputs = torch.cat([\n",
        "            w_flat, g_flat, a_flat, j_flat, l_emb, g_mean, g_std, t_in, t_out, r, c,\n",
        "            dist_center, dist_origin, row_feat, col_feat, h_feat, dL1_feat, dL2_feat,\n",
        "            cos_feat, ce_feat, mag_features, dist_current_center, center_drift_feat,\n",
        "            grad_mag_raw, h_to_g_ratio,\n",
        "        ], dim=1)  # 37\n",
        "\n",
        "        delta = 0.5 * torch.tanh(self.predictor(inputs))\n",
        "        p_full = (w_flat + delta).view_as(weight)\n",
        "\n",
        "        a0 = ALPHA_HEAD_MAX * torch.sigmoid(self.alpha_head(inputs))\n",
        "        return p_full, a0.view_as(weight)\n",
        "\n",
        "def load_hypertuner(ckpt_path: str, use_ema: bool = True) -> FineGrainedHypertuner37:\n",
        "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
        "    if isinstance(ckpt, dict):\n",
        "        if use_ema and (\"ema_state\" in ckpt) and (ckpt[\"ema_state\"] is not None):\n",
        "            state = ckpt[\"ema_state\"]\n",
        "        elif \"model_state\" in ckpt:\n",
        "            state = ckpt[\"model_state\"]\n",
        "        else:\n",
        "            state = ckpt\n",
        "    else:\n",
        "        state = ckpt\n",
        "    ht = FineGrainedHypertuner37(num_layers=2).to(device)\n",
        "    ht.load_state_dict(state, strict=False)\n",
        "    ht.eval()\n",
        "    return ht\n",
        "\n",
        "@torch.no_grad()\n",
        "def apply_step(params_in: List[torch.Tensor], p_full_list: List[torch.Tensor], a_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], float]:\n",
        "    out = []\n",
        "    alpha_means = []\n",
        "    for i, (p, pf, a) in enumerate(zip(params_in, p_full_list, a_list)):\n",
        "        au = torch.clamp(a * mult_for_param(i), 0.0, APPLY_MAX)\n",
        "        out.append(p + au * (pf - p))\n",
        "        alpha_means.append(float(au.mean().item()))\n",
        "    return out, float(np.mean(alpha_means))\n",
        "\n",
        "def step_once(ht: FineGrainedHypertuner37, student: StudentNet, params_in: List[torch.Tensor], x_s: torch.Tensor, y_s: torch.Tensor) -> Tuple[List[torch.Tensor], float]:\n",
        "    ce = support_ce(student, params_in, x_s, y_s)\n",
        "    with torch.enable_grad():\n",
        "        g1, accel, jerk, h0, dL1, dL2, cosv = compute_probe_and_curv(\n",
        "            student, params_in, x_s, y_s,\n",
        "            probe_lr=PROBE_LR,\n",
        "            jerk_scale=JERK_EVAL_SCALE\n",
        "        )\n",
        "\n",
        "    p_full, a_list = [], []\n",
        "    for i, (p, g1i, ai, ji, hi) in enumerate(zip(params_in, g1, accel, jerk, h0)):\n",
        "        pf, a = ht(p, g1i, ai, ji, hi, i // 2, ce, dL1, dL2, cosv)\n",
        "        p_full.append(pf)\n",
        "        a_list.append(a)\n",
        "\n",
        "    out, a_mean = apply_step(params_in, p_full, a_list)\n",
        "    return out, a_mean\n",
        "\n",
        "# -----------------------------\n",
        "# PRETRAIN TO BAND\n",
        "# -----------------------------\n",
        "def sgd_step(student: StudentNet, params: List[torch.Tensor], x: torch.Tensor, y: torch.Tensor, lr: float) -> List[torch.Tensor]:\n",
        "    p_req = [p.detach().clone().requires_grad_(True) for p in params]\n",
        "    logits = student.functional_forward(x, p_req)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    grads = torch.autograd.grad(loss, p_req, create_graph=False, retain_graph=False)\n",
        "    return [p.detach() - lr * g.detach() for p, g in zip(p_req, grads)]\n",
        "\n",
        "def pretrain_until_in_band(\n",
        "    loader: FastMNISTLoader,\n",
        "    student: StudentNet,\n",
        "    center_acc: float,\n",
        "    tol: float,\n",
        "    max_steps: int,\n",
        "    eval_every: int,\n",
        "    lr: float,\n",
        ") -> Tuple[Optional[List[torch.Tensor]], int, float]:\n",
        "    \"\"\"Pretrain until test accuracy enters [centertol].\"\"\"\n",
        "    params = init_params(student)\n",
        "    x_test, y_test = loader.get_full_test()\n",
        "\n",
        "    lo = max(0.0, center_acc - tol)\n",
        "    hi = min(1.0, center_acc + tol)\n",
        "\n",
        "    # fixed SGD minibatch for simplicity (paper: you can mention this)\n",
        "    x_sgd, y_sgd = loader.sample_train(SUPPORT_BATCH)\n",
        "\n",
        "    acc = acc_on_full_test(student, params, x_test, y_test)\n",
        "    if lo <= acc <= hi:\n",
        "        return params, 0, acc\n",
        "\n",
        "    for step in range(1, max_steps + 1):\n",
        "        params = sgd_step(student, params, x_sgd, y_sgd, lr)\n",
        "        if (step % eval_every) == 0:\n",
        "            acc = acc_on_full_test(student, params, x_test, y_test)\n",
        "            if lo <= acc <= hi:\n",
        "                return params, step, acc\n",
        "            if acc > hi + 0.05:\n",
        "                return None, step, acc\n",
        "    return None, max_steps, acc\n",
        "\n",
        "# -----------------------------\n",
        "# MAIN\n",
        "# -----------------------------\n",
        "def main():\n",
        "    print(\"=\" * 110)\n",
        "    print(\" ONE-HOP GSC REFINEMENT CHARACTERIZATION (fixed)\")\n",
        "    print(\"=\" * 110)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Checkpoint: {CHECKPOINT_TO_TEST} (EMA={USE_EMA_WEIGHTS})\")\n",
        "    print(f\"Student hidden: {STUDENT_HIDDEN}\")\n",
        "    print(f\"Targets: {[int(x*100) for x in TARGET_CENTERS]}% {TARGET_TOL*100:.1f}%\")\n",
        "    print(f\"Trials/target: {TRIALS_PER_TARGET}\")\n",
        "    print(\"=\" * 110 + \"\\n\")\n",
        "\n",
        "    loader = FastMNISTLoader(device)\n",
        "    x_test, y_test = loader.get_full_test()\n",
        "\n",
        "    ckpt_path = os.path.join(experiment_dir, \"gold_checkpoints\", CHECKPOINT_TO_TEST)\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        raise FileNotFoundError(f\"Could not find hypertuner checkpoint at: {ckpt_path}\")\n",
        "    ht = load_hypertuner(ckpt_path, use_ema=USE_EMA_WEIGHTS)\n",
        "    print(\" Loaded hypertuner\\n\")\n",
        "\n",
        "    summary = []\n",
        "\n",
        "    for center in TARGET_CENTERS:\n",
        "        lo = max(0.0, center - TARGET_TOL)\n",
        "        hi = min(1.0, center + TARGET_TOL)\n",
        "\n",
        "        print(\"=\" * 110)\n",
        "        print(f\" Target band: {lo*100:.1f}%{hi*100:.1f}% (center {center*100:.0f}%)\")\n",
        "        print(\"-\" * 110)\n",
        "        print(f\"{'Trial':>5} | {'SGD':>4} | {'Before':>8} | {'After':>8} | {'Gain':>8} | {'a':>6} | {'IN?':>4}\")\n",
        "        print(\"-\" * 110)\n",
        "\n",
        "        before, after, gain, steps_used, a_means = [], [], [], [], []\n",
        "        base_lr = PRETRAIN_LR * (0.5 if center <= 0.30 else 1.0)\n",
        "\n",
        "        successes = 0\n",
        "        restarts = 0\n",
        "        while successes < TRIALS_PER_TARGET and restarts < MAX_RESTARTS:\n",
        "            student = StudentNet(hidden=STUDENT_HIDDEN).to(device)\n",
        "            reset_student(student)\n",
        "\n",
        "            params0, sgd_steps, acc0 = pretrain_until_in_band(\n",
        "                loader, student, center, TARGET_TOL,\n",
        "                PRETRAIN_MAX_STEPS, PRETRAIN_EVAL_EVERY, base_lr\n",
        "            )\n",
        "\n",
        "            if params0 is None:\n",
        "                restarts += 1\n",
        "                if acc0 > hi:\n",
        "                    base_lr *= 0.75\n",
        "                continue\n",
        "\n",
        "            x_s, y_s = loader.sample_train(SUPPORT_BATCH)\n",
        "            acc_before = acc_on_full_test(student, params0, x_test, y_test)\n",
        "            params1, a_mean = step_once(ht, student, params0, x_s, y_s)\n",
        "            acc_after = acc_on_full_test(student, params1, x_test, y_test)\n",
        "            g = acc_after - acc_before\n",
        "\n",
        "            in_band = (lo <= acc_before <= hi)\n",
        "\n",
        "            successes += 1\n",
        "            before.append(acc_before); after.append(acc_after); gain.append(g)\n",
        "            steps_used.append(sgd_steps); a_means.append(a_mean)\n",
        "\n",
        "            print(f\"{successes:5d} | {sgd_steps:4d} | {acc_before*100:7.2f}% | {acc_after*100:7.2f}% | {g*100:+7.2f}% | {a_mean:6.3f} | {str(in_band):>4}\")\n",
        "\n",
        "        if len(before) == 0:\n",
        "            print(\"(no successful trials)\\n\")\n",
        "            continue\n",
        "\n",
        "        def mean_std(xs: List[float]) -> Tuple[float, float]:\n",
        "            m = float(np.mean(xs))\n",
        "            s = float(np.std(xs, ddof=1)) if len(xs) > 1 else 0.0\n",
        "            return m, s\n",
        "\n",
        "        b_m, b_s = mean_std(before)\n",
        "        a_m, a_s = mean_std(after)\n",
        "        g_m, g_s = mean_std(gain)\n",
        "        s_m = float(np.mean(steps_used))\n",
        "        alpha_m = float(np.mean(a_means))\n",
        "\n",
        "        print(\"-\" * 110)\n",
        "        print(f\"AVERAGE | SGD={s_m:.1f} | Before={b_m*100:.2f}{b_s*100:.2f} | After={a_m*100:.2f}{a_s*100:.2f} | Gain={g_m*100:+.2f}{g_s*100:.2f} | a={alpha_m:.3f}\\n\")\n",
        "\n",
        "        summary.append((center, b_m, a_m, g_m, s_m, alpha_m))\n",
        "\n",
        "    if summary:\n",
        "        print(\"=\" * 110)\n",
        "        print(\" SUMMARY (ONE-HOP)\")\n",
        "        print(\"=\" * 110)\n",
        "        print(f\"{'Target':>6} | {'Before':>10} | {'After':>10} | {'Gain':>10} | {'SGD':>6} | {'a':>6}\")\n",
        "        print(\"-\" * 110)\n",
        "        for (c, b, a, g, s, am) in summary:\n",
        "            print(f\"{c*100:5.0f}% | {b*100:9.2f}% | {a*100:9.2f}% | {g*100:+9.2f}% | {s:6.1f} | {am:6.3f}\")\n",
        "        print(\"=\" * 110)\n",
        "\n",
        "    print(\"\\n Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9OdoB4x1sub",
        "outputId": "80991ff5-86a3-4da4-c6cd-4a4dbd617a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Mounted at /content/drive\n",
            "==============================================================================================================\n",
            " ONE-HOP GSC REFINEMENT CHARACTERIZATION (fixed)\n",
            "==============================================================================================================\n",
            "Device: cuda\n",
            "Checkpoint: GSC_Gold_OneStep.pth (EMA=True)\n",
            "Student hidden: 96\n",
            "Targets: [20, 30, 40, 50, 60, 70, 80, 90]% 3.0%\n",
            "Trials/target: 10\n",
            "==============================================================================================================\n",
            "\n",
            " Pre-loaded MNIST to GPU\n",
            " Loaded hypertuner\n",
            "\n",
            "==============================================================================================================\n",
            " Target band: 17.0%23.0% (center 20%)\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Trial |  SGD |   Before |    After |     Gain |     a |  IN?\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "    1 |    5 |   22.76% |   96.33% |  +73.57% |  0.893 | True\n",
            "    2 |    5 |   21.48% |   96.15% |  +74.67% |  0.893 | True\n",
            "    3 |    5 |   17.22% |   95.78% |  +78.56% |  0.893 | True\n",
            "    4 |   10 |   17.88% |   95.84% |  +77.96% |  0.893 | True\n",
            "    5 |    0 |   17.85% |   96.08% |  +78.23% |  0.894 | True\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "AVERAGE | SGD=5.0 | Before=19.442.50 | After=96.040.23 | Gain=+76.602.31 | a=0.893\n",
            "\n",
            "==============================================================================================================\n",
            " Target band: 27.0%33.0% (center 30%)\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Trial |  SGD |   Before |    After |     Gain |     a |  IN?\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "    1 |    5 |   31.67% |   95.94% |  +64.27% |  0.894 | True\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "AVERAGE | SGD=5.0 | Before=31.670.00 | After=95.940.00 | Gain=+64.270.00 | a=0.894\n",
            "\n",
            "==============================================================================================================\n",
            " Target band: 37.0%43.0% (center 40%)\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Trial |  SGD |   Before |    After |     Gain |     a |  IN?\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "    1 |    5 |   42.71% |   95.61% |  +52.90% |  0.895 | True\n",
            "    2 |    5 |   39.40% |   95.87% |  +56.47% |  0.893 | True\n",
            "    3 |    5 |   40.07% |   95.60% |  +55.53% |  0.894 | True\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "AVERAGE | SGD=5.0 | Before=40.731.75 | After=95.690.15 | Gain=+54.971.85 | a=0.894\n",
            "\n",
            "==============================================================================================================\n",
            " Target band: 47.0%53.0% (center 50%)\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Trial |  SGD |   Before |    After |     Gain |     a |  IN?\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "    1 |    5 |   48.92% |   95.30% |  +46.38% |  0.893 | True\n",
            "    2 |   10 |   52.46% |   95.75% |  +43.29% |  0.895 | True\n",
            "    3 |   10 |   49.51% |   96.01% |  +46.50% |  0.895 | True\n",
            "    4 |   15 |   51.82% |   95.98% |  +44.16% |  0.895 | True\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "AVERAGE | SGD=10.0 | Before=50.681.73 | After=95.760.33 | Gain=+45.081.61 | a=0.894\n",
            "\n",
            "==============================================================================================================\n",
            " Target band: 57.0%63.0% (center 60%)\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Trial |  SGD |   Before |    After |     Gain |     a |  IN?\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "    1 |    5 |   58.97% |   94.81% |  +35.84% |  0.895 | True\n",
            "    2 |    5 |   58.12% |   95.36% |  +37.24% |  0.894 | True\n",
            "    3 |   15 |   57.88% |   95.73% |  +37.85% |  0.895 | True\n",
            "    4 |   15 |   57.19% |   95.92% |  +38.73% |  0.894 | True\n",
            "    5 |   15 |   62.13% |   95.60% |  +33.47% |  0.895 | True\n",
            "    6 |   20 |   60.58% |   95.59% |  +35.01% |  0.894 | True\n",
            "    7 |   20 |   60.00% |   95.53% |  +35.53% |  0.894 | True\n",
            "    8 |   20 |   57.30% |   95.72% |  +38.42% |  0.895 | True\n",
            "    9 |   20 |   57.74% |   95.57% |  +37.83% |  0.895 | True\n",
            "   10 |   20 |   57.94% |   95.80% |  +37.86% |  0.895 | True\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "AVERAGE | SGD=15.5 | Before=58.781.62 | After=95.560.31 | Gain=+36.781.72 | a=0.895\n",
            "\n",
            "==============================================================================================================\n",
            " Target band: 67.0%73.0% (center 70%)\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Trial |  SGD |   Before |    After |     Gain |     a |  IN?\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "    1 |    5 |   69.99% |   95.84% |  +25.85% |  0.896 | True\n",
            "    2 |    5 |   70.21% |   95.86% |  +25.65% |  0.896 | True\n",
            "    3 |    5 |   68.11% |   95.78% |  +27.67% |  0.895 | True\n",
            "    4 |    5 |   69.36% |   95.61% |  +26.25% |  0.896 | True\n",
            "    5 |    5 |   71.82% |   95.50% |  +23.68% |  0.897 | True\n",
            "    6 |    5 |   69.79% |   96.08% |  +26.29% |  0.897 | True\n",
            "    7 |   10 |   72.63% |   94.81% |  +22.18% |  0.895 | True\n",
            "    8 |   10 |   71.90% |   95.53% |  +23.63% |  0.895 | True\n",
            "    9 |    5 |   67.04% |   95.79% |  +28.75% |  0.895 | True\n",
            "   10 |   10 |   71.12% |   95.60% |  +24.48% |  0.896 | True\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "AVERAGE | SGD=6.5 | Before=70.201.75 | After=95.640.34 | Gain=+25.441.98 | a=0.896\n",
            "\n",
            "==============================================================================================================\n",
            " Target band: 77.0%83.0% (center 80%)\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Trial |  SGD |   Before |    After |     Gain |     a |  IN?\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "    1 |   10 |   79.62% |   95.77% |  +16.15% |  0.896 | True\n",
            "    2 |   10 |   79.62% |   95.43% |  +15.81% |  0.896 | True\n",
            "    3 |   10 |   79.76% |   95.80% |  +16.04% |  0.896 | True\n",
            "    4 |   10 |   79.51% |   95.47% |  +15.96% |  0.896 | True\n",
            "    5 |   10 |   77.92% |   95.77% |  +17.85% |  0.896 | True\n",
            "    6 |   10 |   79.59% |   95.32% |  +15.73% |  0.897 | True\n",
            "    7 |   10 |   79.37% |   95.75% |  +16.38% |  0.897 | True\n",
            "    8 |   10 |   77.42% |   95.28% |  +17.86% |  0.896 | True\n",
            "    9 |   10 |   77.35% |   95.65% |  +18.30% |  0.896 | True\n",
            "   10 |   10 |   77.54% |   95.74% |  +18.20% |  0.896 | True\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "AVERAGE | SGD=10.0 | Before=78.771.06 | After=95.600.20 | Gain=+16.831.08 | a=0.896\n",
            "\n",
            "==============================================================================================================\n",
            " Target band: 87.0%93.0% (center 90%)\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Trial |  SGD |   Before |    After |     Gain |     a |  IN?\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "    1 |   50 |   87.20% |   94.35% |   +7.15% |  0.895 | True\n",
            "    2 |   40 |   87.22% |   95.08% |   +7.86% |  0.896 | True\n",
            "    3 |   45 |   87.00% |   95.70% |   +8.70% |  0.898 | True\n",
            "    4 |   50 |   87.12% |   95.52% |   +8.40% |  0.897 | True\n",
            "    5 |   50 |   87.08% |   95.01% |   +7.93% |  0.897 | True\n",
            "    6 |   40 |   87.23% |   95.23% |   +8.00% |  0.896 | True\n",
            "    7 |   70 |   87.04% |   95.19% |   +8.15% |  0.898 | True\n",
            "    8 |   35 |   87.35% |   95.43% |   +8.08% |  0.897 | True\n",
            "    9 |   35 |   87.05% |   95.22% |   +8.17% |  0.896 | True\n",
            "   10 |   45 |   87.01% |   95.46% |   +8.45% |  0.897 | True\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "AVERAGE | SGD=46.0 | Before=87.130.12 | After=95.220.37 | Gain=+8.090.42 | a=0.897\n",
            "\n",
            "==============================================================================================================\n",
            " SUMMARY (ONE-HOP)\n",
            "==============================================================================================================\n",
            "Target |     Before |      After |       Gain |    SGD |     a\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "   20% |     19.44% |     96.04% |    +76.60% |    5.0 |  0.893\n",
            "   30% |     31.67% |     95.94% |    +64.27% |    5.0 |  0.894\n",
            "   40% |     40.73% |     95.69% |    +54.97% |    5.0 |  0.894\n",
            "   50% |     50.68% |     95.76% |    +45.08% |   10.0 |  0.894\n",
            "   60% |     58.78% |     95.56% |    +36.78% |   15.5 |  0.895\n",
            "   70% |     70.20% |     95.64% |    +25.44% |    6.5 |  0.896\n",
            "   80% |     78.77% |     95.60% |    +16.83% |   10.0 |  0.896\n",
            "   90% |     87.13% |     95.22% |     +8.09% |   46.0 |  0.897\n",
            "==============================================================================================================\n",
            "\n",
            " Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================\n",
        "# COMPREHENSIVE FEATURE ABLATION TEST\n",
        "#\n",
        "# Tests which features the hypertuner actually uses:\n",
        "# 1. Random features - does it ignore inputs entirely?\n",
        "# 2. Individual ablations - which features matter?\n",
        "# 3. Weight ablation - does it need current weights?\n",
        "# ============================================================\n",
        "\n",
        "torch.set_grad_enabled(True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\" Running on {device}\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "experiment_dir = '/content/drive/My Drive/GradientShortCircuit_Experiments'\n",
        "CHECKPOINT_TO_TEST = 'GSC_Gold_OneStep.pth'\n",
        "USE_EMA_WEIGHTS = True\n",
        "\n",
        "NUM_TRIALS = 20\n",
        "SUPPORT_BATCH = 1024\n",
        "PROBE_LR = 0.02\n",
        "\n",
        "# Features\n",
        "ALPHA_HEAD_MAX = 0.5\n",
        "APPLY_MAX = 1.5\n",
        "W0, B0, W1, B1 = 1.8, 1.6, 2.0, 2.2\n",
        "DELTAL_SCALE = 2.0\n",
        "DELTAL2_SCALE = 5.0\n",
        "JERK_EVAL_SCALE = 1e-4\n",
        "\n",
        "def mult_for_param(i: int) -> float:\n",
        "    return [W0, B0, W1, B1][i]\n",
        "\n",
        "# -----------------------------\n",
        "# DATA\n",
        "# -----------------------------\n",
        "class FastMNISTLoader:\n",
        "    def __init__(self, device):\n",
        "        print(f\" Pre-loading MNIST to GPU VRAM...\")\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "        train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "        test_data  = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "        self.device = device\n",
        "        self.train_x = train_data.data.view(-1, 784).float().to(device) / 255.0\n",
        "        self.train_y = train_data.targets.to(device)\n",
        "        self.test_x  = test_data.data.view(-1, 784).float().to(device) / 255.0\n",
        "        self.test_y  = test_data.targets.to(device)\n",
        "\n",
        "        self.train_x = (self.train_x - 0.1307) / 0.3081\n",
        "        self.test_x  = (self.test_x  - 0.1307) / 0.3081\n",
        "        self.num_train = self.train_x.shape[0]\n",
        "\n",
        "    def sample_train(self, batch_size):\n",
        "        idx = torch.randint(0, self.num_train, (batch_size,), device=self.device)\n",
        "        return self.train_x[idx], self.train_y[idx]\n",
        "\n",
        "    def get_full_test(self):\n",
        "        return self.test_x, self.test_y\n",
        "\n",
        "# -----------------------------\n",
        "# STUDENT\n",
        "# -----------------------------\n",
        "class MNISTStudent(nn.Module):\n",
        "    def __init__(self, hidden_size=96):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(784, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def functional_forward(self, x, params):\n",
        "        x = F.linear(x, params[0], params[1])\n",
        "        x = F.relu(x)\n",
        "        x = F.linear(x, params[2], params[3])\n",
        "        return x\n",
        "\n",
        "def reset_student(net):\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "# -----------------------------\n",
        "# FEATURES\n",
        "# -----------------------------\n",
        "def soft_squash(x, eps=1e-8):\n",
        "    s = x.detach().abs().median().clamp(min=eps)\n",
        "    return torch.tanh(x / (3.0 * s))\n",
        "\n",
        "def signed_log1p(x):\n",
        "    return torch.sign(x) * torch.log1p(x.abs())\n",
        "\n",
        "def compute_probe_and_curv(student, params, x_support, y_support, probe_lr=0.02, jerk_scale=1.0):\n",
        "    p0 = [p.detach().clone().requires_grad_(True) for p in params]\n",
        "    logits1 = student.functional_forward(x_support, p0)\n",
        "    loss1 = F.cross_entropy(logits1, y_support)\n",
        "    grads1 = torch.autograd.grad(loss1, p0, create_graph=False)\n",
        "    g1 = [g.detach() for g in grads1]\n",
        "    h_diag = [g.pow(2) for g in g1]\n",
        "\n",
        "    p1 = [(pp - probe_lr * gg).detach().requires_grad_(True) for pp, gg in zip(p0, g1)]\n",
        "    logits2 = student.functional_forward(x_support, p1)\n",
        "    loss2 = F.cross_entropy(logits2, y_support)\n",
        "    deltaL1 = float((loss2 - loss1).detach().item())\n",
        "    grads2 = torch.autograd.grad(loss2, p1, create_graph=False)\n",
        "    g2 = [g.detach() for g in grads2]\n",
        "    accel = [g2i - g1i for g1i, g2i in zip(g1, g2)]\n",
        "\n",
        "    eps = 1e-12\n",
        "    dot = torch.zeros((), device=g1[0].device)\n",
        "    n1  = torch.zeros((), device=g1[0].device)\n",
        "    n2  = torch.zeros((), device=g1[0].device)\n",
        "    for a, b in zip(g1, g2):\n",
        "        aa = a.reshape(-1)\n",
        "        bb = b.reshape(-1)\n",
        "        dot = dot + (aa * bb).sum()\n",
        "        n1  = n1  + (aa * aa).sum()\n",
        "        n2  = n2  + (bb * bb).sum()\n",
        "    cosv = float((dot / (torch.sqrt(n1).clamp_min(eps) * torch.sqrt(n2).clamp_min(eps))).detach().item())\n",
        "\n",
        "    p2 = [(pp - probe_lr * gg).detach().requires_grad_(True) for pp, gg in zip(p1, g2)]\n",
        "    logits3 = student.functional_forward(x_support, p2)\n",
        "    loss3 = F.cross_entropy(logits3, y_support)\n",
        "    deltaL2_raw = float((loss3 - loss2).detach().item())\n",
        "    grads3 = torch.autograd.grad(loss3, p2, create_graph=False)\n",
        "    g3 = [g.detach() for g in grads3]\n",
        "    jerk = [jerk_scale * (g3i - 2.0*g2i + g1i) for g1i, g2i, g3i in zip(g1, g2, g3)]\n",
        "\n",
        "    return g1, accel, jerk, h_diag, deltaL1, deltaL2_raw, cosv\n",
        "\n",
        "# -----------------------------\n",
        "# HYPERTUNER\n",
        "# -----------------------------\n",
        "class FineGrainedHypertuner37(nn.Module):\n",
        "    def __init__(self, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.layer_embeddings = nn.Embedding(num_layers, 10)\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(37, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        self.alpha_head = nn.Sequential(\n",
        "            nn.Linear(37, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, weight, grad_g1, grad_accel, grad_jerk, h_diag, layer_idx,\n",
        "                support_ce_scalar, deltaL1_scalar, deltaL2_scalar, cos_scalar,\n",
        "                ablate_feature=None):\n",
        "        \"\"\"\n",
        "        ablate_feature: str or None\n",
        "            If specified, zero out that feature group before processing\n",
        "            Options: 'weight', 'gradient', 'accel', 'jerk', 'hessian',\n",
        "                     'deltaL', 'cos', 'ce', 'magnitude', 'geometry', 'all'\n",
        "        \"\"\"\n",
        "        batch_size = weight.numel()\n",
        "        w_flat = weight.reshape(-1, 1)\n",
        "        g_flat = soft_squash(grad_g1).reshape(-1, 1)\n",
        "        a_flat = soft_squash(grad_accel).reshape(-1, 1)\n",
        "        j_flat = soft_squash(grad_jerk).reshape(-1, 1)\n",
        "        h_feat = soft_squash(signed_log1p(h_diag)).reshape(-1, 1)\n",
        "\n",
        "        dL1_feat = torch.tanh(torch.full((batch_size, 1), float(deltaL1_scalar) * DELTAL_SCALE,\n",
        "                                        device=weight.device, dtype=w_flat.dtype))\n",
        "        dL2_feat = torch.tanh(torch.full((batch_size, 1), float(deltaL2_scalar) * DELTAL2_SCALE,\n",
        "                                        device=weight.device, dtype=w_flat.dtype))\n",
        "        cos_feat = torch.full((batch_size, 1), float(cos_scalar), device=weight.device, dtype=w_flat.dtype)\n",
        "        ce_feat  = torch.full((batch_size, 1), float(support_ce_scalar), device=weight.device, dtype=w_flat.dtype)\n",
        "\n",
        "        log_mag_g1    = float(torch.log1p(grad_g1.detach().abs().median()).item())\n",
        "        log_mag_accel = float(torch.log1p(grad_accel.detach().abs().median()).item())\n",
        "        log_mag_jerk  = float(torch.log1p(grad_jerk.detach().abs().median()).item())\n",
        "        log_mag_h     = float(torch.log1p(h_diag.detach().abs().median()).item())\n",
        "        mag_features = torch.tensor([log_mag_g1, log_mag_accel, log_mag_jerk, log_mag_h],\n",
        "                                    device=weight.device, dtype=w_flat.dtype).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        current_center = float(weight.mean().item())\n",
        "        dist_current_center = torch.log1p((w_flat - current_center).abs())\n",
        "        center_drift = torch.log1p(torch.abs(torch.tensor(current_center, device=weight.device)))\n",
        "        center_drift_feat = torch.full((batch_size, 1), float(center_drift.item()),\n",
        "                                      device=weight.device, dtype=w_flat.dtype)\n",
        "        grad_mag_raw = torch.log1p(grad_g1.abs()).reshape(-1, 1)\n",
        "        h_to_g_ratio = soft_squash(h_diag.abs() / (grad_g1.abs() + 1e-8)).reshape(-1, 1)\n",
        "\n",
        "        if weight.dim() > 1:\n",
        "            rows, cols = weight.shape\n",
        "            r_idx = torch.arange(rows, device=weight.device)\n",
        "            c_idx = torch.arange(cols, device=weight.device)\n",
        "            r = r_idx.repeat_interleave(cols).float().reshape(-1, 1) / rows\n",
        "            c = c_idx.repeat(rows).float().reshape(-1, 1) / cols\n",
        "\n",
        "            center_row, center_col = (rows - 1) / 2.0, (cols - 1) / 2.0\n",
        "            rr = r_idx.repeat_interleave(cols).float()\n",
        "            cc = c_idx.repeat(rows).float()\n",
        "            dist = torch.sqrt((rr - center_row) ** 2 + (cc - center_col) ** 2)\n",
        "            max_dist = torch.sqrt(torch.tensor(center_row**2 + center_col**2,\n",
        "                                               device=weight.device, dtype=dist.dtype)).clamp(min=1e-8)\n",
        "            dist_center = (dist / max_dist).reshape(-1, 1)\n",
        "\n",
        "            fan_in = max(int(cols), 1)\n",
        "            sigma = torch.sqrt(torch.tensor(2.0 / fan_in, device=weight.device, dtype=w_flat.dtype)).clamp(min=1e-8)\n",
        "            dist_origin = torch.log1p(w_flat.abs() / sigma)\n",
        "\n",
        "            row_norms = grad_g1.norm(dim=1, keepdim=True)\n",
        "            col_norms = grad_g1.norm(dim=0, keepdim=True).t()\n",
        "            row_feat = soft_squash(row_norms.repeat_interleave(cols, dim=0))\n",
        "            col_feat = soft_squash(col_norms.repeat(rows, 1))\n",
        "\n",
        "            g_mean = g_flat.mean().expand(batch_size, 1)\n",
        "            g_std  = g_flat.std().expand(batch_size, 1)\n",
        "\n",
        "            t_in  = torch.full((batch_size, 1), float(np.log1p(cols)), device=weight.device, dtype=w_flat.dtype)\n",
        "            t_out = torch.full((batch_size, 1), float(np.log1p(rows)), device=weight.device, dtype=w_flat.dtype)\n",
        "        else:\n",
        "            rows = weight.shape[0]\n",
        "            r_idx = torch.arange(rows, device=weight.device)\n",
        "            r = r_idx.float().reshape(-1, 1) / rows\n",
        "            c = torch.zeros_like(r)\n",
        "\n",
        "            center = (rows - 1) / 2.0\n",
        "            dist = (r_idx.float() - center).abs()\n",
        "            max_dist = torch.tensor(center, device=weight.device, dtype=dist.dtype).clamp(min=1e-8)\n",
        "            dist_center = (dist / max_dist).reshape(-1, 1)\n",
        "\n",
        "            fan_in = 784 if layer_idx == 0 else 96\n",
        "            sigma = torch.sqrt(torch.tensor(2.0 / fan_in, device=weight.device, dtype=w_flat.dtype)).clamp(min=1e-8)\n",
        "            dist_origin = torch.log1p(w_flat.abs() / sigma)\n",
        "\n",
        "            row_feat = soft_squash(grad_g1.abs().reshape(-1, 1))\n",
        "            col_feat = torch.zeros_like(row_feat)\n",
        "\n",
        "            g_mean = g_flat.mean().expand(batch_size, 1)\n",
        "            g_std  = g_flat.std().expand(batch_size, 1)\n",
        "\n",
        "            t_in  = torch.zeros((batch_size, 1), device=weight.device, dtype=w_flat.dtype)\n",
        "            t_out = torch.full((batch_size, 1), float(np.log1p(rows)), device=weight.device, dtype=w_flat.dtype)\n",
        "\n",
        "        l_emb = self.layer_embeddings(torch.tensor([layer_idx], device=weight.device)).expand(batch_size, -1)\n",
        "\n",
        "        # Apply ablation if specified\n",
        "        if ablate_feature == 'weight':\n",
        "            w_flat = torch.zeros_like(w_flat)\n",
        "            dist_current_center = torch.zeros_like(dist_current_center)\n",
        "            center_drift_feat = torch.zeros_like(center_drift_feat)\n",
        "            dist_origin = torch.zeros_like(dist_origin)\n",
        "        elif ablate_feature == 'gradient':\n",
        "            g_flat = torch.zeros_like(g_flat)\n",
        "            g_mean = torch.zeros_like(g_mean)\n",
        "            g_std = torch.zeros_like(g_std)\n",
        "            row_feat = torch.zeros_like(row_feat)\n",
        "            col_feat = torch.zeros_like(col_feat)\n",
        "            grad_mag_raw = torch.zeros_like(grad_mag_raw)\n",
        "        elif ablate_feature == 'accel':\n",
        "            a_flat = torch.zeros_like(a_flat)\n",
        "        elif ablate_feature == 'jerk':\n",
        "            j_flat = torch.zeros_like(j_flat)\n",
        "        elif ablate_feature == 'hessian':\n",
        "            h_feat = torch.zeros_like(h_feat)\n",
        "            h_to_g_ratio = torch.zeros_like(h_to_g_ratio)\n",
        "        elif ablate_feature == 'deltaL':\n",
        "            dL1_feat = torch.zeros_like(dL1_feat)\n",
        "            dL2_feat = torch.zeros_like(dL2_feat)\n",
        "        elif ablate_feature == 'cos':\n",
        "            cos_feat = torch.zeros_like(cos_feat)\n",
        "        elif ablate_feature == 'ce':\n",
        "            ce_feat = torch.zeros_like(ce_feat)\n",
        "        elif ablate_feature == 'magnitude':\n",
        "            mag_features = torch.zeros_like(mag_features)\n",
        "        elif ablate_feature == 'geometry':\n",
        "            r = torch.zeros_like(r)\n",
        "            c = torch.zeros_like(c)\n",
        "            dist_center = torch.zeros_like(dist_center)\n",
        "            t_in = torch.zeros_like(t_in)\n",
        "            t_out = torch.zeros_like(t_out)\n",
        "\n",
        "        inputs = torch.cat([\n",
        "            w_flat, g_flat, a_flat, j_flat, l_emb, g_mean, g_std, t_in, t_out, r, c,\n",
        "            dist_center, dist_origin, row_feat, col_feat, h_feat, dL1_feat, dL2_feat,\n",
        "            cos_feat, ce_feat, mag_features, dist_current_center, center_drift_feat,\n",
        "            grad_mag_raw, h_to_g_ratio,\n",
        "        ], dim=1)\n",
        "\n",
        "        delta = 0.5 * torch.tanh(self.predictor(inputs))\n",
        "        p_full = (w_flat + delta).view_as(weight)\n",
        "\n",
        "        a0 = ALPHA_HEAD_MAX * torch.sigmoid(self.alpha_head(inputs))\n",
        "        return p_full, a0.view_as(weight)\n",
        "\n",
        "# -----------------------------\n",
        "# HELPERS\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def acc_on_full_test(student, params, x_test, y_test):\n",
        "    return (student.functional_forward(x_test, params).argmax(1) == y_test).float().mean().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def support_ce(student, x_s, y_s, params):\n",
        "    return float(F.cross_entropy(student.functional_forward(x_s, params), y_s).item())\n",
        "\n",
        "def apply_step(params_in, p_full_list, a_list, mult_scale=1.0):\n",
        "    out = []\n",
        "    for i, (p, pf, a) in enumerate(zip(params_in, p_full_list, a_list)):\n",
        "        au = torch.clamp(a * mult_for_param(i) * mult_scale, 0.0, APPLY_MAX)\n",
        "        out.append(p + au * (pf - p))\n",
        "    return out\n",
        "\n",
        "def step_once(ht, student, params_in, x_s, y_s, ablate_feature=None):\n",
        "    \"\"\"Apply hypertuner with optional feature ablation\"\"\"\n",
        "    ce = support_ce(student, x_s, y_s, params_in)\n",
        "\n",
        "    with torch.enable_grad():\n",
        "        g1, accel, jerk, h0, dL1, dL2, cosv = compute_probe_and_curv(\n",
        "            student, params_in, x_s, y_s,\n",
        "            probe_lr=PROBE_LR,\n",
        "            jerk_scale=JERK_EVAL_SCALE\n",
        "        )\n",
        "\n",
        "    p_full, a_list = [], []\n",
        "    for i, (p, g1i, ai, ji, hi) in enumerate(zip(params_in, g1, accel, jerk, h0)):\n",
        "        pf, a = ht(p, g1i, ai, ji, hi, i // 2, ce, dL1, dL2, cosv,\n",
        "                  ablate_feature=ablate_feature)\n",
        "        p_full.append(pf)\n",
        "        a_list.append(a)\n",
        "\n",
        "    out = apply_step(params_in, p_full, a_list, mult_scale=1.0)\n",
        "    return out\n",
        "\n",
        "# -----------------------------\n",
        "# LOAD HYPERTUNER\n",
        "# -----------------------------\n",
        "def load_hypertuner(ckpt_path, use_ema=True):\n",
        "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
        "    if isinstance(ckpt, dict):\n",
        "        if use_ema and (\"ema_state\" in ckpt) and (ckpt[\"ema_state\"] is not None):\n",
        "            state = ckpt[\"ema_state\"]\n",
        "        elif \"model_state\" in ckpt:\n",
        "            state = ckpt[\"model_state\"]\n",
        "        else:\n",
        "            state = ckpt\n",
        "    else:\n",
        "        state = ckpt\n",
        "\n",
        "    ht = FineGrainedHypertuner37(num_layers=2).to(device)\n",
        "    ht.load_state_dict(state, strict=False)\n",
        "    ht.eval()\n",
        "    return ht\n",
        "\n",
        "# -----------------------------\n",
        "# TESTS\n",
        "# -----------------------------\n",
        "def test_random_features(ht, student, fast_loader, trials=20):\n",
        "    \"\"\"Test: Real features vs completely random features\"\"\"\n",
        "    print(\"=\"*100)\n",
        "    print(\" TEST 1: Real Features vs Random Features\")\n",
        "    print(\"=\"*100)\n",
        "    print(\"Tests if hypertuner uses gradient/feature inputs or ignores them entirely\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    x_test, y_test = fast_loader.get_full_test()\n",
        "\n",
        "    real_accs = []\n",
        "    random_accs = []\n",
        "\n",
        "    for trial in range(trials):\n",
        "        reset_student(student)\n",
        "        student.eval()\n",
        "        params0 = [p.detach().clone() for p in student.parameters()]\n",
        "        x_s, y_s = fast_loader.sample_train(SUPPORT_BATCH)\n",
        "\n",
        "        # Real features\n",
        "        with torch.no_grad():\n",
        "            params_real = step_once(ht, student, params0, x_s, y_s, ablate_feature=None)\n",
        "        acc_real = acc_on_full_test(student, params_real, x_test, y_test)\n",
        "\n",
        "        # Random features - replace ALL gradient-based features with noise\n",
        "        with torch.enable_grad():\n",
        "            g1, accel, jerk, h0, dL1, dL2, cosv = compute_probe_and_curv(\n",
        "                student, params0, x_s, y_s, probe_lr=PROBE_LR, jerk_scale=JERK_EVAL_SCALE\n",
        "            )\n",
        "\n",
        "        # Randomize all gradient features\n",
        "        g1_rand = [torch.randn_like(g) for g in g1]\n",
        "        accel_rand = [torch.randn_like(a) for a in accel]\n",
        "        jerk_rand = [torch.randn_like(j) for j in jerk]\n",
        "        h0_rand = [torch.randn_like(h) for h in h0]\n",
        "        dL1_rand = np.random.randn()\n",
        "        dL2_rand = np.random.randn()\n",
        "        cos_rand = np.random.randn()\n",
        "        ce_rand = np.random.randn()\n",
        "\n",
        "        p_full_rand, a_list_rand = [], []\n",
        "        for i, (p, g1i, ai, ji, hi) in enumerate(zip(params0, g1_rand, accel_rand, jerk_rand, h0_rand)):\n",
        "            pf, a = ht(p, g1i, ai, ji, hi, i // 2, ce_rand, dL1_rand, dL2_rand, cos_rand)\n",
        "            p_full_rand.append(pf)\n",
        "            a_list_rand.append(a)\n",
        "\n",
        "        params_rand = apply_step(params0, p_full_rand, a_list_rand, mult_scale=1.0)\n",
        "        acc_rand = acc_on_full_test(student, params_rand, x_test, y_test)\n",
        "\n",
        "        real_accs.append(acc_real)\n",
        "        random_accs.append(acc_rand)\n",
        "\n",
        "        print(f\"Trial {trial+1:2d} | Real: {acc_real*100:6.2f}% | Random: {acc_rand*100:6.2f}% | \"\n",
        "              f\"Diff: {(acc_real-acc_rand)*100:+6.2f}%\")\n",
        "\n",
        "    print(\"-\" * 100)\n",
        "    mean_diff = (np.mean(real_accs) - np.mean(random_accs)) * 100\n",
        "    print(f\"AVERAGE   | Real: {np.mean(real_accs)*100:6.2f}% | Random: {np.mean(random_accs)*100:6.2f}% | \"\n",
        "          f\"Diff: {mean_diff:+6.2f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    if abs(mean_diff) < 5.0:\n",
        "        print(\"  WARNING: Real  Random! Hypertuner might IGNORE gradient features!\")\n",
        "    elif mean_diff > 50.0:\n",
        "        print(\" STRONG EVIDENCE: Hypertuner USES gradient features! (Real >> Random)\")\n",
        "    else:\n",
        "        print(\" Hypertuner uses gradient features, but effect is moderate\")\n",
        "    print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "    return {'real': np.mean(real_accs), 'random': np.mean(random_accs), 'diff': mean_diff}\n",
        "\n",
        "def test_individual_ablations(ht, student, fast_loader, trials=10):\n",
        "    \"\"\"Test: Ablate each feature group individually\"\"\"\n",
        "    print(\"=\"*100)\n",
        "    print(\" TEST 2: Individual Feature Ablations\")\n",
        "    print(\"=\"*100)\n",
        "    print(\"Tests which feature groups are most important by zeroing each out\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    x_test, y_test = fast_loader.get_full_test()\n",
        "\n",
        "    feature_groups = [\n",
        "        ('baseline', None),\n",
        "        ('no_weight', 'weight'),\n",
        "        ('no_gradient', 'gradient'),\n",
        "        ('no_accel', 'accel'),\n",
        "        ('no_jerk', 'jerk'),\n",
        "        ('no_hessian', 'hessian'),\n",
        "        ('no_deltaL', 'deltaL'),\n",
        "        ('no_cos', 'cos'),\n",
        "        ('no_ce', 'ce'),\n",
        "        ('no_magnitude', 'magnitude'),\n",
        "        ('no_geometry', 'geometry'),\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, ablate in feature_groups:\n",
        "        accs = []\n",
        "        for trial in range(trials):\n",
        "            reset_student(student)\n",
        "            student.eval()\n",
        "            params0 = [p.detach().clone() for p in student.parameters()]\n",
        "            x_s, y_s = fast_loader.sample_train(SUPPORT_BATCH)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                params = step_once(ht, student, params0, x_s, y_s, ablate_feature=ablate)\n",
        "            acc = acc_on_full_test(student, params, x_test, y_test)\n",
        "            accs.append(acc)\n",
        "\n",
        "        mean_acc = np.mean(accs)\n",
        "        std_acc = np.std(accs)\n",
        "        results[name] = {'mean': mean_acc, 'std': std_acc}\n",
        "\n",
        "        if name == 'baseline':\n",
        "            print(f\"{'Baseline (all features)':<25} | {mean_acc*100:6.2f}%  {std_acc*100:4.2f}% | [reference]\")\n",
        "        else:\n",
        "            diff = (results['baseline']['mean'] - mean_acc) * 100\n",
        "            print(f\"{name:<25} | {mean_acc*100:6.2f}%  {std_acc*100:4.2f}% | Drop: {diff:+6.2f}%\")\n",
        "\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    # Rank by importance\n",
        "    baseline_acc = results['baseline']['mean']\n",
        "    importance = []\n",
        "    for name, ablate in feature_groups[1:]:  # Skip baseline\n",
        "        drop = (baseline_acc - results[name]['mean']) * 100\n",
        "        importance.append((name, drop))\n",
        "\n",
        "    importance.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(\"\\n Feature Importance Ranking (by accuracy drop when ablated):\")\n",
        "    print(\"-\" * 100)\n",
        "    for i, (name, drop) in enumerate(importance, 1):\n",
        "        if drop > 10.0:\n",
        "            status = \" CRITICAL\"\n",
        "        elif drop > 3.0:\n",
        "            status = \" Important\"\n",
        "        elif drop > 1.0:\n",
        "            status = \"  Moderate\"\n",
        "        else:\n",
        "            status = \" Minimal\"\n",
        "        print(f\"{i:2d}. {name:<25} | Drop: {drop:+6.2f}% | {status}\")\n",
        "\n",
        "    print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "    return results, importance\n",
        "\n",
        "def plot_ablation_results(results, importance):\n",
        "    \"\"\"Plot the ablation results\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Plot 1: Accuracy with each ablation\n",
        "    names = [name for name, _ in [('baseline', None)] + importance]\n",
        "    means = [results[name]['mean'] * 100 for name in names]\n",
        "    stds = [results[name]['std'] * 100 for name in names]\n",
        "\n",
        "    colors = ['green'] + ['red' if drop > 10 else 'orange' if drop > 3 else 'blue'\n",
        "                          for _, drop in importance]\n",
        "\n",
        "    bars = ax1.barh(range(len(names)), means, xerr=stds, color=colors, alpha=0.7)\n",
        "    ax1.set_yticks(range(len(names)))\n",
        "    ax1.set_yticklabels(names)\n",
        "    ax1.set_xlabel('Accuracy (%)', fontsize=12)\n",
        "    ax1.set_title('Accuracy with Feature Ablations', fontsize=14, fontweight='bold')\n",
        "    ax1.axvline(x=means[0], color='green', linestyle='--', linewidth=2, alpha=0.5, label='Baseline')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "    # Plot 2: Drop from baseline\n",
        "    drops = [drop for _, drop in importance]\n",
        "    names_sorted = [name for name, _ in importance]\n",
        "\n",
        "    colors2 = ['red' if d > 10 else 'orange' if d > 3 else 'blue' for d in drops]\n",
        "\n",
        "    ax2.barh(range(len(drops)), drops, color=colors2, alpha=0.7)\n",
        "    ax2.set_yticks(range(len(drops)))\n",
        "    ax2.set_yticklabels(names_sorted)\n",
        "    ax2.set_xlabel('Accuracy Drop (%)', fontsize=12)\n",
        "    ax2.set_title('Feature Importance (Drop when Ablated)', fontsize=14, fontweight='bold')\n",
        "    ax2.axvline(x=10, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Critical (>10%)')\n",
        "    ax2.axvline(x=3, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='Important (>3%)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# MAIN\n",
        "# -----------------------------\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\" COMPREHENSIVE FEATURE ABLATION TEST\")\n",
        "    print(\"=\"*100)\n",
        "    print(\"Testing which features the hypertuner actually uses\")\n",
        "    print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "    # Load data\n",
        "    fast_loader = FastMNISTLoader(device)\n",
        "    x_test, y_test = fast_loader.get_full_test()\n",
        "\n",
        "    # Load hypertuner\n",
        "    ckpt_path = os.path.join(experiment_dir, 'gold_checkpoints', CHECKPOINT_TO_TEST)\n",
        "    ht = load_hypertuner(ckpt_path, use_ema=USE_EMA_WEIGHTS)\n",
        "    print(f\" Loaded hypertuner: {CHECKPOINT_TO_TEST}\\n\")\n",
        "\n",
        "    # Create student\n",
        "    student = MNISTStudent(hidden_size=96).to(device)\n",
        "\n",
        "    # Run tests\n",
        "    print(\"\\n\")\n",
        "    random_results = test_random_features(ht, student, fast_loader, trials=NUM_TRIALS)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    ablation_results, importance = test_individual_ablations(ht, student, fast_loader, trials=10)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    plot_ablation_results(ablation_results, importance)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\" Ablation tests complete!\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n SUMMARY:\")\n",
        "    print(\"-\" * 100)\n",
        "    print(f\"Random features test: {random_results['diff']:+.1f}% difference\")\n",
        "    if random_results['diff'] > 50:\n",
        "        print(\"   Hypertuner STRONGLY depends on gradient features\")\n",
        "    elif random_results['diff'] > 10:\n",
        "        print(\"   Hypertuner uses gradient features\")\n",
        "    else:\n",
        "        print(\"   WARNING: Hypertuner may be memorizing!\")\n",
        "\n",
        "    print(\"\\nMost important features:\")\n",
        "    for i, (name, drop) in enumerate(importance[:5], 1):\n",
        "        print(f\"  {i}. {name}: -{drop:.1f}% when removed\")\n",
        "\n",
        "    print(\"=\"*100)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4vRshOSXJVvR",
        "outputId": "96fe97e7-5bf2-4a88-f5fe-854604f17784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Running on cuda\n",
            "Mounted at /content/drive\n",
            "\n",
            "====================================================================================================\n",
            " COMPREHENSIVE FEATURE ABLATION TEST\n",
            "====================================================================================================\n",
            "Testing which features the hypertuner actually uses\n",
            "====================================================================================================\n",
            "\n",
            " Pre-loading MNIST to GPU VRAM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9.91M/9.91M [00:01<00:00, 5.60MB/s]\n",
            "100%|| 28.9k/28.9k [00:00<00:00, 132kB/s]\n",
            "100%|| 1.65M/1.65M [00:01<00:00, 1.22MB/s]\n",
            "100%|| 4.54k/4.54k [00:00<00:00, 7.22MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loaded hypertuner: GSC_Gold_OneStep.pth\n",
            "\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            " TEST 1: Real Features vs Random Features\n",
            "====================================================================================================\n",
            "Tests if hypertuner uses gradient/feature inputs or ignores them entirely\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Trial  1 | Real:  95.93% | Random:  10.76% | Diff: +85.17%\n",
            "Trial  2 | Real:  96.03% | Random:  11.32% | Diff: +84.71%\n",
            "Trial  3 | Real:  96.01% | Random:   9.82% | Diff: +86.19%\n",
            "Trial  4 | Real:  96.21% | Random:   9.98% | Diff: +86.23%\n",
            "Trial  5 | Real:  96.14% | Random:  11.13% | Diff: +85.01%\n",
            "Trial  6 | Real:  96.34% | Random:  10.44% | Diff: +85.90%\n",
            "Trial  7 | Real:  96.00% | Random:  10.18% | Diff: +85.82%\n",
            "Trial  8 | Real:  96.26% | Random:  10.11% | Diff: +86.15%\n",
            "Trial  9 | Real:  95.91% | Random:   9.94% | Diff: +85.97%\n",
            "Trial 10 | Real:  96.20% | Random:   9.82% | Diff: +86.38%\n",
            "Trial 11 | Real:  96.26% | Random:   9.99% | Diff: +86.27%\n",
            "Trial 12 | Real:  96.11% | Random:   9.85% | Diff: +86.26%\n",
            "Trial 13 | Real:  96.06% | Random:  10.37% | Diff: +85.69%\n",
            "Trial 14 | Real:  96.06% | Random:   9.99% | Diff: +86.07%\n",
            "Trial 15 | Real:  96.17% | Random:   9.96% | Diff: +86.21%\n",
            "Trial 16 | Real:  96.31% | Random:   9.88% | Diff: +86.43%\n",
            "Trial 17 | Real:  95.91% | Random:  11.14% | Diff: +84.77%\n",
            "Trial 18 | Real:  96.17% | Random:   9.83% | Diff: +86.34%\n",
            "Trial 19 | Real:  96.21% | Random:  10.26% | Diff: +85.95%\n",
            "Trial 20 | Real:  96.10% | Random:  10.75% | Diff: +85.35%\n",
            "----------------------------------------------------------------------------------------------------\n",
            "AVERAGE   | Real:  96.12% | Random:  10.28% | Diff: +85.84%\n",
            "\n",
            "====================================================================================================\n",
            " STRONG EVIDENCE: Hypertuner USES gradient features! (Real >> Random)\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            " TEST 2: Individual Feature Ablations\n",
            "====================================================================================================\n",
            "Tests which feature groups are most important by zeroing each out\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Baseline (all features)   |  96.09%  0.12% | [reference]\n",
            "no_weight                 |  94.94%  0.20% | Drop:  +1.15%\n",
            "no_gradient               |  10.76%  2.52% | Drop: +85.34%\n",
            "no_accel                  |  96.07%  0.17% | Drop:  +0.02%\n",
            "no_jerk                   |  96.12%  0.06% | Drop:  -0.02%\n",
            "no_hessian                |  96.05%  0.10% | Drop:  +0.05%\n",
            "no_deltaL                 |  95.52%  0.14% | Drop:  +0.58%\n",
            "no_cos                    |  94.78%  0.30% | Drop:  +1.31%\n",
            "no_ce                     |  95.81%  0.15% | Drop:  +0.28%\n",
            "no_magnitude              |  95.98%  0.13% | Drop:  +0.12%\n",
            "no_geometry               |  10.36%  0.75% | Drop: +85.73%\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            " Feature Importance Ranking (by accuracy drop when ablated):\n",
            "----------------------------------------------------------------------------------------------------\n",
            " 1. no_geometry               | Drop: +85.73% |  CRITICAL\n",
            " 2. no_gradient               | Drop: +85.34% |  CRITICAL\n",
            " 3. no_cos                    | Drop:  +1.31% |   Moderate\n",
            " 4. no_weight                 | Drop:  +1.15% |   Moderate\n",
            " 5. no_deltaL                 | Drop:  +0.58% |  Minimal\n",
            " 6. no_ce                     | Drop:  +0.28% |  Minimal\n",
            " 7. no_magnitude              | Drop:  +0.12% |  Minimal\n",
            " 8. no_hessian                | Drop:  +0.05% |  Minimal\n",
            " 9. no_accel                  | Drop:  +0.02% |  Minimal\n",
            "10. no_jerk                   | Drop:  -0.02% |  Minimal\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            " Ablation tests complete!\n",
            "====================================================================================================\n",
            "\n",
            " SUMMARY:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Random features test: +85.8% difference\n",
            "   Hypertuner STRONGLY depends on gradient features\n",
            "\n",
            "Most important features:\n",
            "  1. no_geometry: -85.7% when removed\n",
            "  2. no_gradient: -85.3% when removed\n",
            "  3. no_cos: -1.3% when removed\n",
            "  4. no_weight: -1.2% when removed\n",
            "  5. no_deltaL: -0.6% when removed\n",
            "====================================================================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1KIofgoryFauR-5k2QriWgkwlBZS7CHaS",
      "authorship_tag": "ABX9TyNnZ6jBkKbsQ2kyU59CwPQi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}